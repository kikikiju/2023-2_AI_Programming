{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 감정분석 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. 데이터 수집\n",
    "## 1-1. csv파일로 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문장</th>\n",
       "      <th>감정</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>어, 청소 니가 대신 해 줘!</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>둘 다 청소 하기 싫어. 귀찮아.</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>둘 다 하기 싫어서 화내.</td>\n",
       "      <td>Angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>그럼 방세는 어떡해.</td>\n",
       "      <td>Sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>권태긴줄 알았는데 다른 사람이 생겼나보더라고.</td>\n",
       "      <td>Sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43986</th>\n",
       "      <td>나 엘리베이터에 갇혔어.</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43987</th>\n",
       "      <td>하지만 기분이 나쁜 걸 어떡해?</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43988</th>\n",
       "      <td>자취방 엘리베이턴데 정전인가봐.</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43989</th>\n",
       "      <td>나 드디어 프로젝트 끝났어!</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43990</th>\n",
       "      <td>걱정해줘서 고마워.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43991 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              문장         감정\n",
       "0               어, 청소 니가 대신 해 줘!    Neutral\n",
       "1             둘 다 청소 하기 싫어. 귀찮아.    Neutral\n",
       "2                 둘 다 하기 싫어서 화내.      Angry\n",
       "3                    그럼 방세는 어떡해.    Sadness\n",
       "4      권태긴줄 알았는데 다른 사람이 생겼나보더라고.    Sadness\n",
       "...                          ...        ...\n",
       "43986              나 엘리베이터에 갇혔어.  happiness\n",
       "43987          하지만 기분이 나쁜 걸 어떡해?    sadness\n",
       "43988          자취방 엘리베이턴데 정전인가봐.    sadness\n",
       "43989            나 드디어 프로젝트 끝났어!    disgust\n",
       "43990                 걱정해줘서 고마워.    neutral\n",
       "\n",
       "[43991 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data_ori.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral' 'angry' 'sadness' 'disgust' 'surprise' 'fear' 'happiness']\n"
     ]
    }
   ],
   "source": [
    "df.loc[:, '감정'] = df['감정'].str.lower()\n",
    "unique_count = df['감정'].unique()\n",
    "print(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['angry' 'sadness' 'disgust' 'surprise' 'fear' 'happiness']\n"
     ]
    }
   ],
   "source": [
    "df = df[df['감정'] != 'neutral']\n",
    "unique_count = df['감정'].unique()\n",
    "print(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_copy = df.copy()\n",
    "\n",
    "#감정을 숫자로 매칭\n",
    "df_copy.loc[(df_copy['감정'] == \"angry\"), '감정'] = 0\n",
    "df_copy.loc[(df_copy['감정'] == \"disgust\"), '감정'] = 0\n",
    "df_copy.loc[(df_copy['감정'] == \"sadness\"), '감정'] = 1\n",
    "df_copy.loc[(df_copy['감정'] == \"fear\"), '감정'] = 2\n",
    "df_copy.loc[(df_copy['감정'] == \"surprise\"), '감정'] = 2\n",
    "df_copy.loc[(df_copy['감정'] == \"happiness\"), '감정'] = 3\n",
    "\n",
    "# 원래 df에 다시 넣어주고, 맨 왼쪽의 인덱스를 초기화해줌\n",
    "df.loc[:, '감정'] = df_copy['감정']\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감정\n",
      "1    13415\n",
      "0    10080\n",
      "2     5029\n",
      "3     3509\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_count = df['감정'].value_counts()\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새로 조절한 클래스별 갯수:\n",
      "0: 3509개\n",
      "1: 3509개\n",
      "2: 3509개\n",
      "3: 3509개\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "target_list = []\n",
    "\n",
    "for q,  label in zip(df['문장'], df['감정']):\n",
    "    data_list.append(q)\n",
    "    target_list.append(label)\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# all_target_num에서 각 클래스별 개수 확인\n",
    "class_counts = Counter(target_list)\n",
    "\n",
    "# 가장 적은 데이터 수를 가진 클래스의 수\n",
    "min_samples = min(class_counts.values())\n",
    "\n",
    "# 새로운 데이터와 타겟을 저장할 리스트\n",
    "distribute_data = []\n",
    "distribute_target = []\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 각 클래스에서 가장 적은 샘플 수만큼 데이터를 선택하여 저장\n",
    "for target_class in set(target_list):\n",
    "    class_indices = [i for i, c in enumerate(target_list) if c == target_class]\n",
    "    selected_indices = np.random.choice(class_indices, min_samples, replace=False)\n",
    "    distribute_data.extend([data_list[i] for i in selected_indices])\n",
    "    distribute_target.extend([target_list[i] for i in selected_indices])\n",
    "\n",
    "# 리스트를 numpy 배열로 변환\n",
    "distribute_data = np.array(distribute_data)\n",
    "new_all_target_num = np.array(distribute_target)\n",
    "\n",
    "# 클래스별로 데이터 수 확인 (새로 조절한 데이터로 확인)\n",
    "distribute_class_counts = Counter(distribute_target)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"새로 조절한 클래스별 갯수:\")\n",
    "for emotion, count in distribute_class_counts.items():\n",
    "    print(f\"{emotion}: {count}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "\n",
    "kkma = Kkma()\n",
    "\n",
    "tokenized_data = []\n",
    "\n",
    "for s in distribute_data:\n",
    "    text_2 = [word for word, pos in kkma.pos(s) if (pos == 'NNG') or (pos == 'VV') or (pos == 'NP') or (pos == 'VA') or (pos == 'MAG')] # 조사 또는 문장부호는 빼줌\n",
    "    #if text_2:  # 빈 리스트가 아닌 경우에만 추가\n",
    "    tokenized_data.append(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('응', 'UN'), ('.', 'SF'), ('쓰레기', 'NNG'), ('는', 'JX'), ('버리', 'VV'), ('ㄴ다', 'ECS'), ('치', 'VV'), ('어도', 'ECD'), ('쓰레기통', 'NNG'), ('에서', 'JKM'), ('냄새', 'NNG'), ('가', 'JKS'), ('안', 'MAG'), ('빠지', 'VV'), ('어', 'ECS'), ('.', 'SF')]\n",
      "응. 쓰레기는 버린다쳐도 쓰레기통에서 냄새가 안 빠져.\n",
      "['쓰레기', '버리', '치', '쓰레기통', '냄새', '안', '빠지']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(kkma.pos(distribute_data[0]))\n",
    "print(distribute_data[0])\n",
    "print(tokenized_data[0])\n",
    "print(distribute_target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 전체 데이터에 대한 단어 빈도수 계산\n",
    "word_counts = Counter(word for sentence in tokenized_data for word in sentence)\n",
    "\n",
    "# 빈도수에 따라 단어를 정렬\n",
    "sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 상위 N개의 단어로 단어 사전 만들기\n",
    "top_words = [word for word, count in sorted_words[:500]]\n",
    "word_to_index = {word: idx for idx, word in enumerate(top_words)}\n",
    "\n",
    "# 모든 데이터를 숫자로 변환\n",
    "num_tokenized_data = [[word_to_index.get(word, 0) for word in sentence] for sentence in tokenized_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나': 0, '하': 1, '너무': 2, '되': 3, '안': 4, '없': 5, '좋': 6, '있': 7, '같': 8, '오': 9, '알': 10, '친구': 11, '정말': 12, '보': 13, '그리하': 14, '고맙': 15, '일': 16, '그러': 17, '또': 18, '어제': 19, '진짜': 20, '힘들': 21, '지금': 22, '사람': 23, '기분': 24, '무섭': 25, '가': 26, '내': 27, '집': 28, '다': 29, '많이': 30, '약속': 31, '어': 32, '당첨': 33, '해피': 34, '좀': 35, '놀라': 36, '못': 37, '듣': 38, '아': 39, '먹': 40, '내가': 41, '끝나': 42, '아직': 43, '회사': 44, '다치': 45, '엄청': 46, '생각': 47, '이벤트': 48, '청소': 49, '그렇': 50, '괜찮': 51, '마음': 52, '갑자기': 53, '시간': 54, '때': 55, '잘': 56, '화장실': 57, '모르': 58, '오늘': 59, '프로젝트': 60, '맞': 61, '나오': 62, '드디어': 63, '갇히': 64, '같애': 65, '받': 66, '우리': 67, '어떻': 68, '말': 69, '만나': 70, '깜짝': 71, '떨어지': 72, '그냥': 73, '요즘': 74, '지진': 75, '걱정': 76, '그': 77, '나가': 78, '축하': 79, '연락': 80, '저녁': 81, '이제': 82, '않': 83, '곰팡이': 84, '술': 85, '많': 86, '뭐': 87, '싸우': 88, '전화': 89, '치우': 90, '왜': 91, '자': 92, '마라톤': 93, '냄새': 94, '산책': 95, '헤어지': 96, '달': 97, '근데': 98, '동안': 99, '아빠': 100, '기다리': 101, '생기': 102, '이번': 103, '크': 104, '기록': 105, '밥': 106, '계속': 107, '너': 108, '아까': 109, '얼마나': 110, '얘기': 111, '다시': 112, '혼자': 113, '같이': 114, '더': 115, '퇴근': 116, '드시': 117, '면접': 118, '조금': 119, '살': 120, '향수': 121, '내일': 122, '해보': 123, '놀래': 124, '좋아하': 125, '버리': 126, '자주': 127, '헐': 128, '밖': 129, '그래서': 130, '준비': 131, '물': 132, '대회': 133, '음악': 134, '방': 135, '우울': 136, '이야기': 137, '그대로': 138, '결국': 139, '구경': 140, '아프': 141, '화': 142, '완전': 143, '완전히': 144, '뽑': 145, '누': 146, '병원': 147, '다행히': 148, '달리': 149, '늦': 150, '짜증나': 151, '강남': 152, '매일': 153, '쓰': 154, '쓰레기통': 155, '갖': 156, '주': 157, '선물': 158, '일어나': 159, '막': 160, '넘어지': 161, '곳': 162, '가족': 163, '신나': 164, '지치': 165, '있었': 166, '기획': 167, '몰리': 168, '사': 169, '허리': 170, '목줄': 171, '세상': 172, '주식': 173, '테': 174, '아니': 175, '잘하': 176, '쉬': 177, '몸': 178, '떠나': 179, '견': 180, '나한': 181, '잠': 182, '처음': 183, '토하': 184, '쓰레기': 185, '결승선': 186, '매번': 187, '이렇': 188, '자꾸': 189, '너무나': 190, '추천': 191, '전': 192, '주변': 193, '갱신': 194, '당연히': 195, '노트북': 196, '바쁘': 197, '더럽': 198, '유기': 199, '벌레': 200, '그거': 201, '새끼': 202, '속상하': 203, '말하': 204, '정도': 205, '죽': 206, '룸메이트': 207, '거기': 208, '끊어지': 209, '힘': 210, '휴가': 211, '순간': 212, '뛰': 213, '통과': 214, '역겹': 215, '들어오': 216, '소리': 217, '낳': 218, '확': 219, '짜장면': 220, '없었': 221, '느': 222, '음식물': 223, '말씀': 224, '드리': 225, '들': 226, '항상': 227, '까': 228, '도착': 229, '나도': 230, '싫': 231, '진정': 232, '궁금하': 233, '맨날': 234, '아무래도': 235, '짜증': 236, '갈': 237, '비싸': 238, '안하': 239, '마시': 240, '비': 241, '지나': 242, '룸': 243, '시키': 244, '벌써': 245, '이러': 246, '빨리': 247, '진자': 248, '시작하': 249, '인': 250, '남': 251, '토': 252, '날': 253, '길': 254, '일주일': 255, '다음': 256, '그것': 257, '식당': 258, '넘': 259, '다니': 260, '천둥': 261, '드': 262, '어떡하': 263, '코로나': 264, '진행': 265, '꼭': 266, '영화': 267, '한번': 268, '비가': 269, '안되': 270, '잡': 271, '권태기': 272, '부모님': 273, '출전': 274, '여행': 275, '고등학교': 276, '함께': 277, '잘못': 278, '고치': 279, '열심히': 280, '언서': 281, '치': 282, '야': 283, '크루': 284, '스트레스': 285, '상사': 286, '선수': 287, '제거제': 288, '야근': 289, '끝내': 290, '경찰': 291, '계단': 292, '사라지': 293, '메': 294, '구매': 295, '요새': 296, '예전': 297, '운동': 298, '무기력': 299, '참': 300, '문제': 301, '바로': 302, '다녀오': 303, '들어가': 304, '나아지': 305, '밤': 306, '몰르': 307, '오래': 308, '깨': 309, '다르': 310, '돈': 311, '보이': 312, '피': 313, '놀': 314, '보여주': 315, '망하': 316, '치진': 317, '운': 318, '아주': 319, '키우': 320, '잠깐': 321, '찾': 322, '훨씬': 323, '당첨자': 324, '아침': 325, '사과': 326, '그런데': 327, '엘리베이터': 328, '동네': 329, '가도': 330, '발표': 331, '일찍': 332, '거지': 333, '슬프': 334, '난리': 335, '이것': 336, '괜히': 337, '발목': 338, '효과': 339, '하루': 340, '딱': 341, '어떡': 342, '이상': 343, '찍': 344, '지지': 345, '고민': 346, '나중': 347, '우산': 348, '역': 349, '가지': 350, '가보': 351, '미루': 352, '한데': 353, '사진': 354, '며칠': 355, '버티': 356, '다행': 357, '자랑': 358, '놓': 359, '심하': 360, '새롭': 361, '제출': 362, '장소': 363, '좋아지': 364, '위': 365, '욕': 366, '아무': 367, '실망': 368, '강아지': 369, '고생': 370, '하나': 371, '오르': 372, '시작': 373, '다큐멘터리': 374, '앞': 375, '이거': 376, '상황': 377, '방향제': 378, '최종': 379, '혼나': 380, '엄마': 381, '손해': 382, '영상': 383, '층': 384, '오랜만': 385, '맛있': 386, '이해': 387, '화가': 388, '마': 389, '여기저기': 390, '슬퍼하': 391, '업무': 392, '얼른': 393, '니': 394, '종일': 395, '방법': 396, '너무하': 397, '지장': 398, '고': 399, '남자': 400, '치료': 401, '싹': 402, '감염병': 403, '아무것': 404, '미안': 405, '마지막': 406, '책임감': 407, '무엇': 408, '거의': 409, '그럼': 410, '원래': 411, '환기': 412, '이직': 413, '갔다오': 414, '최근': 415, '낫': 416, '별로': 417, '맘': 418, '옆': 419, '노력': 420, '상하': 421, '생명': 422, '계시': 423, '이렇게': 424, '속': 425, '동창': 426, '따로': 427, '당장': 428, '해지': 429, '엉망': 430, '일하': 431, '그리고': 432, '의식': 433, '풀리': 434, '그때': 435, '줄': 436, '행복': 437, '적이': 438, '여자': 439, '지역': 440, '입원': 441, '어지럽히': 442, '유명': 443, '스': 444, '서로': 445, '걷': 446, '입맛': 447, '광고': 448, '이래': 449, '전혀': 450, '아이': 451, '빼': 452, '옮기': 453, '오시': 454, '뒷': 455, '조심': 456, '기대': 457, '응급실': 458, '확인': 459, '노래': 460, '끊': 461, '덜': 462, '문자': 463, '일단': 464, '날씨': 465, '쉽': 466, '잊히': 467, '두렵': 468, '우울하': 469, '너무너무': 470, '마치': 471, '포기': 472, '아마': 473, '신경': 474, '지': 475, '어렵': 476, '별': 477, '그만두': 478, '잃': 479, '가슴': 480, '생활비': 481, '빠지': 482, '나쁘': 483, '사용': 484, '계획': 485, '바람': 486, '만들': 487, '바닥': 488, '믿': 489, '저': 490, '그저께': 491, '부족': 492, '반찬': 493, '들어주': 494, '달려가': 495, '향': 496, '직장': 497, '얼마': 498, '평소': 499}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나: 3310번\n",
      "하: 2515번\n",
      "너무: 2010번\n",
      "되: 1532번\n",
      "안: 1436번\n",
      "없: 1220번\n",
      "좋: 1220번\n",
      "있: 1189번\n",
      "같: 984번\n",
      "오: 795번\n",
      "알: 787번\n",
      "친구: 723번\n",
      "정말: 716번\n",
      "보: 716번\n",
      "그리하: 649번\n",
      "고맙: 627번\n",
      "일: 623번\n",
      "그러: 621번\n",
      "또: 621번\n",
      "어제: 613번\n",
      "진짜: 606번\n",
      "힘들: 588번\n",
      "지금: 575번\n",
      "사람: 568번\n",
      "기분: 568번\n",
      "무섭: 556번\n",
      "가: 534번\n",
      "내: 533번\n",
      "집: 521번\n",
      "다: 520번\n",
      "많이: 502번\n",
      "약속: 477번\n",
      "어: 456번\n",
      "당첨: 451번\n",
      "해피: 434번\n",
      "좀: 432번\n",
      "놀라: 429번\n",
      "못: 417번\n",
      "듣: 412번\n",
      "아: 411번\n",
      "먹: 405번\n",
      "내가: 403번\n",
      "끝나: 401번\n",
      "아직: 400번\n",
      "회사: 400번\n",
      "다치: 394번\n",
      "엄청: 381번\n",
      "생각: 378번\n",
      "이벤트: 373번\n",
      "청소: 359번\n",
      "그렇: 352번\n",
      "괜찮: 336번\n",
      "마음: 335번\n",
      "갑자기: 333번\n",
      "시간: 329번\n",
      "때: 326번\n",
      "잘: 320번\n",
      "화장실: 312번\n",
      "모르: 312번\n",
      "오늘: 311번\n",
      "프로젝트: 305번\n",
      "맞: 299번\n",
      "나오: 298번\n",
      "드디어: 287번\n",
      "갇히: 281번\n",
      "같애: 280번\n",
      "받: 272번\n",
      "우리: 271번\n",
      "어떻: 270번\n",
      "말: 264번\n",
      "만나: 263번\n",
      "깜짝: 260번\n",
      "떨어지: 259번\n",
      "그냥: 251번\n",
      "요즘: 250번\n",
      "지진: 248번\n",
      "걱정: 246번\n",
      "그: 245번\n",
      "나가: 243번\n",
      "축하: 241번\n",
      "연락: 240번\n",
      "저녁: 239번\n",
      "이제: 238번\n",
      "않: 234번\n",
      "곰팡이: 233번\n",
      "술: 231번\n",
      "많: 231번\n",
      "뭐: 225번\n",
      "싸우: 221번\n",
      "전화: 216번\n",
      "치우: 214번\n",
      "왜: 211번\n",
      "자: 211번\n",
      "마라톤: 209번\n",
      "냄새: 206번\n",
      "산책: 206번\n",
      "헤어지: 205번\n",
      "달: 205번\n",
      "근데: 205번\n",
      "동안: 204번\n",
      "아빠: 203번\n",
      "기다리: 202번\n",
      "생기: 199번\n",
      "이번: 198번\n",
      "크: 198번\n",
      "기록: 198번\n",
      "밥: 196번\n",
      "계속: 195번\n",
      "너: 194번\n",
      "아까: 193번\n",
      "얼마나: 192번\n",
      "얘기: 191번\n",
      "다시: 190번\n",
      "혼자: 190번\n",
      "같이: 190번\n",
      "더: 189번\n",
      "퇴근: 188번\n",
      "드시: 188번\n",
      "면접: 187번\n",
      "조금: 185번\n",
      "살: 184번\n",
      "향수: 184번\n",
      "내일: 183번\n",
      "해보: 181번\n",
      "놀래: 181번\n",
      "좋아하: 178번\n",
      "버리: 176번\n",
      "자주: 176번\n",
      "헐: 168번\n",
      "밖: 167번\n",
      "그래서: 167번\n",
      "준비: 160번\n",
      "물: 159번\n",
      "대회: 158번\n",
      "음악: 155번\n",
      "방: 154번\n",
      "우울: 154번\n",
      "이야기: 153번\n",
      "그대로: 147번\n",
      "결국: 146번\n",
      "구경: 144번\n",
      "아프: 143번\n",
      "화: 139번\n",
      "완전: 139번\n",
      "완전히: 139번\n",
      "뽑: 139번\n",
      "누: 138번\n",
      "병원: 138번\n",
      "다행히: 138번\n",
      "달리: 138번\n",
      "늦: 135번\n",
      "짜증나: 134번\n",
      "강남: 133번\n",
      "매일: 132번\n",
      "쓰: 131번\n",
      "쓰레기통: 129번\n",
      "갖: 129번\n",
      "주: 128번\n",
      "선물: 128번\n",
      "일어나: 127번\n",
      "막: 127번\n",
      "넘어지: 127번\n",
      "곳: 127번\n",
      "가족: 126번\n",
      "신나: 126번\n",
      "지치: 125번\n",
      "있었: 125번\n",
      "기획: 122번\n",
      "몰리: 121번\n",
      "사: 117번\n",
      "허리: 117번\n",
      "목줄: 117번\n",
      "세상: 117번\n",
      "주식: 116번\n",
      "테: 115번\n",
      "아니: 115번\n",
      "잘하: 113번\n",
      "쉬: 113번\n",
      "몸: 113번\n",
      "떠나: 113번\n",
      "견: 111번\n",
      "나한: 110번\n",
      "잠: 110번\n",
      "처음: 109번\n",
      "토하: 108번\n",
      "쓰레기: 106번\n",
      "결승선: 106번\n",
      "매번: 105번\n",
      "이렇: 104번\n",
      "자꾸: 104번\n",
      "너무나: 104번\n",
      "추천: 104번\n",
      "전: 103번\n",
      "주변: 103번\n",
      "갱신: 103번\n",
      "당연히: 101번\n",
      "노트북: 101번\n",
      "바쁘: 100번\n",
      "더럽: 100번\n",
      "유기: 100번\n",
      "벌레: 100번\n",
      "그거: 100번\n",
      "새끼: 99번\n",
      "속상하: 97번\n",
      "말하: 96번\n",
      "정도: 96번\n",
      "죽: 95번\n",
      "룸메이트: 95번\n",
      "거기: 94번\n",
      "끊어지: 94번\n",
      "힘: 94번\n",
      "휴가: 93번\n",
      "순간: 93번\n",
      "뛰: 93번\n",
      "통과: 93번\n",
      "역겹: 92번\n",
      "들어오: 91번\n",
      "소리: 91번\n",
      "낳: 90번\n",
      "확: 90번\n",
      "짜장면: 89번\n",
      "없었: 89번\n",
      "느: 88번\n",
      "음식물: 88번\n",
      "말씀: 88번\n",
      "드리: 88번\n",
      "들: 87번\n",
      "항상: 87번\n",
      "까: 87번\n",
      "도착: 87번\n",
      "나도: 87번\n",
      "싫: 86번\n",
      "진정: 86번\n",
      "궁금하: 86번\n",
      "맨날: 85번\n",
      "아무래도: 85번\n",
      "짜증: 83번\n",
      "갈: 83번\n",
      "비싸: 83번\n",
      "안하: 82번\n",
      "마시: 82번\n",
      "비: 82번\n",
      "지나: 82번\n",
      "룸: 82번\n",
      "시키: 81번\n",
      "벌써: 80번\n",
      "이러: 80번\n",
      "빨리: 80번\n",
      "진자: 80번\n",
      "시작하: 80번\n",
      "인: 80번\n",
      "남: 79번\n",
      "토: 78번\n",
      "날: 78번\n",
      "길: 77번\n",
      "일주일: 77번\n",
      "다음: 77번\n",
      "그것: 77번\n",
      "식당: 77번\n",
      "넘: 76번\n",
      "다니: 76번\n",
      "천둥: 76번\n",
      "드: 76번\n",
      "어떡하: 75번\n",
      "코로나: 75번\n",
      "진행: 74번\n",
      "꼭: 74번\n",
      "영화: 74번\n",
      "한번: 73번\n",
      "비가: 73번\n",
      "안되: 72번\n",
      "잡: 72번\n",
      "권태기: 72번\n",
      "부모님: 72번\n",
      "출전: 72번\n",
      "여행: 70번\n",
      "고등학교: 70번\n",
      "함께: 70번\n",
      "잘못: 69번\n",
      "고치: 68번\n",
      "열심히: 68번\n",
      "언서: 68번\n",
      "치: 66번\n",
      "야: 66번\n",
      "크루: 66번\n",
      "스트레스: 65번\n",
      "상사: 65번\n",
      "선수: 65번\n",
      "제거제: 64번\n",
      "야근: 64번\n",
      "끝내: 64번\n",
      "경찰: 64번\n",
      "계단: 63번\n",
      "사라지: 63번\n",
      "메: 62번\n",
      "구매: 62번\n",
      "요새: 61번\n",
      "예전: 61번\n",
      "운동: 60번\n",
      "무기력: 60번\n",
      "참: 59번\n",
      "문제: 59번\n",
      "바로: 59번\n",
      "다녀오: 59번\n",
      "들어가: 58번\n",
      "나아지: 58번\n",
      "밤: 58번\n",
      "몰르: 58번\n",
      "오래: 58번\n",
      "깨: 58번\n",
      "다르: 57번\n",
      "돈: 57번\n",
      "보이: 57번\n",
      "피: 57번\n",
      "놀: 57번\n",
      "보여주: 56번\n",
      "망하: 56번\n",
      "치진: 56번\n",
      "운: 56번\n",
      "아주: 55번\n",
      "키우: 55번\n",
      "잠깐: 55번\n",
      "찾: 55번\n",
      "훨씬: 55번\n",
      "당첨자: 55번\n",
      "아침: 54번\n",
      "사과: 54번\n",
      "그런데: 54번\n",
      "엘리베이터: 54번\n",
      "동네: 54번\n",
      "가도: 54번\n",
      "발표: 54번\n",
      "일찍: 53번\n",
      "거지: 53번\n",
      "슬프: 53번\n",
      "난리: 53번\n",
      "이것: 53번\n",
      "괜히: 53번\n",
      "발목: 53번\n",
      "효과: 52번\n",
      "하루: 52번\n",
      "딱: 52번\n",
      "어떡: 52번\n",
      "이상: 51번\n",
      "찍: 51번\n",
      "지지: 51번\n",
      "고민: 51번\n",
      "나중: 51번\n",
      "우산: 50번\n",
      "역: 50번\n",
      "가지: 50번\n",
      "가보: 50번\n",
      "미루: 50번\n",
      "한데: 49번\n",
      "사진: 49번\n",
      "며칠: 49번\n",
      "버티: 49번\n",
      "다행: 49번\n",
      "자랑: 49번\n",
      "놓: 48번\n",
      "심하: 48번\n",
      "새롭: 48번\n",
      "제출: 48번\n",
      "장소: 48번\n",
      "좋아지: 47번\n",
      "위: 47번\n",
      "욕: 46번\n",
      "아무: 46번\n",
      "실망: 46번\n",
      "강아지: 46번\n",
      "고생: 46번\n",
      "하나: 46번\n",
      "오르: 46번\n",
      "시작: 45번\n",
      "다큐멘터리: 45번\n",
      "앞: 45번\n",
      "이거: 45번\n",
      "상황: 45번\n",
      "방향제: 45번\n",
      "최종: 45번\n",
      "혼나: 44번\n",
      "엄마: 44번\n",
      "손해: 44번\n",
      "영상: 43번\n",
      "층: 43번\n",
      "오랜만: 43번\n",
      "맛있: 43번\n",
      "이해: 42번\n",
      "화가: 42번\n",
      "마: 42번\n",
      "여기저기: 42번\n",
      "슬퍼하: 42번\n",
      "업무: 41번\n",
      "얼른: 41번\n",
      "니: 41번\n",
      "종일: 41번\n",
      "방법: 41번\n",
      "너무하: 40번\n",
      "지장: 40번\n",
      "고: 40번\n",
      "남자: 40번\n",
      "치료: 40번\n",
      "싹: 40번\n",
      "감염병: 40번\n",
      "아무것: 39번\n",
      "미안: 39번\n",
      "마지막: 39번\n",
      "책임감: 38번\n",
      "무엇: 38번\n",
      "거의: 38번\n",
      "그럼: 38번\n",
      "원래: 38번\n",
      "환기: 38번\n",
      "이직: 37번\n",
      "갔다오: 37번\n",
      "최근: 37번\n",
      "낫: 37번\n",
      "별로: 37번\n",
      "맘: 37번\n",
      "옆: 37번\n",
      "노력: 37번\n",
      "상하: 36번\n",
      "생명: 36번\n",
      "계시: 36번\n",
      "이렇게: 36번\n",
      "속: 36번\n",
      "동창: 36번\n",
      "따로: 36번\n",
      "당장: 36번\n",
      "해지: 35번\n",
      "엉망: 35번\n",
      "일하: 35번\n",
      "그리고: 35번\n",
      "의식: 35번\n",
      "풀리: 35번\n",
      "그때: 34번\n",
      "줄: 34번\n",
      "행복: 34번\n",
      "적이: 34번\n",
      "여자: 34번\n",
      "지역: 34번\n",
      "입원: 33번\n",
      "어지럽히: 33번\n",
      "유명: 33번\n",
      "스: 33번\n",
      "서로: 33번\n",
      "걷: 33번\n",
      "입맛: 33번\n",
      "광고: 32번\n",
      "이래: 32번\n",
      "전혀: 32번\n",
      "아이: 32번\n",
      "빼: 32번\n",
      "옮기: 31번\n",
      "오시: 31번\n",
      "뒷: 31번\n",
      "조심: 31번\n",
      "기대: 31번\n",
      "응급실: 31번\n",
      "확인: 31번\n",
      "노래: 31번\n",
      "끊: 30번\n",
      "덜: 30번\n",
      "문자: 30번\n",
      "일단: 30번\n",
      "날씨: 30번\n",
      "쉽: 30번\n",
      "잊히: 30번\n",
      "두렵: 30번\n",
      "우울하: 30번\n",
      "너무너무: 30번\n",
      "마치: 30번\n",
      "포기: 29번\n",
      "아마: 29번\n",
      "신경: 29번\n",
      "지: 29번\n",
      "어렵: 29번\n",
      "별: 29번\n",
      "그만두: 29번\n",
      "잃: 29번\n",
      "가슴: 29번\n",
      "생활비: 29번\n",
      "빠지: 28번\n",
      "나쁘: 28번\n",
      "사용: 28번\n",
      "계획: 28번\n",
      "바람: 28번\n",
      "만들: 28번\n",
      "바닥: 28번\n",
      "믿: 28번\n",
      "저: 28번\n",
      "그저께: 28번\n",
      "부족: 28번\n",
      "반찬: 28번\n",
      "들어주: 28번\n",
      "달려가: 28번\n",
      "향: 28번\n",
      "직장: 27번\n",
      "얼마: 27번\n",
      "평소: 27번\n"
     ]
    }
   ],
   "source": [
    "# 상위 500개의 단어 선택\n",
    "top_words = [word for word, count in word_counts.most_common(500)]\n",
    "\n",
    "# 상위 500개의 단어에 대한 빈도수 출력\n",
    "for word in top_words:\n",
    "    print(f'{word}: {word_counts[word]}번')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26, 451, 28, 134, 0], [30, 7, 10, 88, 38, 4, 9], [0, 153, 195, 4, 18, 0, 63], [0, 178, 169, 139, 290], [66, 25, 280, 7, 12, 72, 37, 341, 60, 280, 12, 430, 0], [342, 482, 5, 286], [207, 270, 131, 0], [129, 1, 0, 8, 52, 291, 0, 2, 163, 0, 142], [370, 189, 4, 0, 12, 14, 179, 170, 14, 0], [29, 0, 0, 10, 371, 344, 225, 176, 64]]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(num_tokenized_data[0:10])\n",
    "print(distribute_target[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.481761185522941 6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "lengths = np.array([len(x) for x in num_tokenized_data])\n",
    "print(np.mean(lengths), np.median(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(lengths)\n",
    "plt.xlabel(\"length\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kikikiju\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "(14036, 7)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "train_seq = pad_sequences(num_tokenized_data, maxlen = 7)\n",
    "\n",
    "print(train_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_input, test_input, train_target, test_target = train_test_split(train_seq, distribute_target, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, val_input, train_target, val_target = train_test_split(train_input, train_target, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 7)\n",
      "(2246, 7)\n",
      "(2808, 7)\n",
      "(8982,)\n",
      "(2246,)\n",
      "(2808,)\n"
     ]
    }
   ],
   "source": [
    "train_target = np.array(train_target)\n",
    "val_target = np.array(val_target)\n",
    "test_target = np.array(test_target)\n",
    "\n",
    "print(train_input.shape)\n",
    "print(val_input.shape)\n",
    "print(test_input.shape)\n",
    "\n",
    "print(train_target.shape)\n",
    "print(val_target.shape)\n",
    "print(test_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. 모델\n",
    "\n",
    "## 2-1. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "141/141 [==============================] - 7s 21ms/step - loss: 1.5480 - accuracy: 0.3546 - val_loss: 1.3865 - val_accuracy: 0.2346\n",
      "Epoch 2/300\n",
      "141/141 [==============================] - 2s 14ms/step - loss: 1.2867 - accuracy: 0.4290 - val_loss: 1.3367 - val_accuracy: 0.3246\n",
      "Epoch 3/300\n",
      "141/141 [==============================] - 2s 14ms/step - loss: 1.1983 - accuracy: 0.4715 - val_loss: 1.2370 - val_accuracy: 0.4653\n",
      "Epoch 4/300\n",
      "141/141 [==============================] - 2s 14ms/step - loss: 1.1360 - accuracy: 0.5031 - val_loss: 1.0918 - val_accuracy: 0.5516\n",
      "Epoch 5/300\n",
      "141/141 [==============================] - 2s 13ms/step - loss: 1.0975 - accuracy: 0.5184 - val_loss: 1.0103 - val_accuracy: 0.5801\n",
      "Epoch 6/300\n",
      "141/141 [==============================] - 2s 14ms/step - loss: 1.0817 - accuracy: 0.5356 - val_loss: 0.9541 - val_accuracy: 0.6073\n",
      "Epoch 7/300\n",
      "141/141 [==============================] - 2s 14ms/step - loss: 1.0484 - accuracy: 0.5598 - val_loss: 0.9277 - val_accuracy: 0.6220\n",
      "Epoch 8/300\n",
      "141/141 [==============================] - 2s 14ms/step - loss: 1.0171 - accuracy: 0.5743 - val_loss: 0.9114 - val_accuracy: 0.6300\n",
      "Epoch 9/300\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 1.0078 - accuracy: 0.5845 - val_loss: 0.8997 - val_accuracy: 0.6318\n",
      "Epoch 10/300\n",
      "141/141 [==============================] - 3s 23ms/step - loss: 0.9781 - accuracy: 0.5963 - val_loss: 0.8781 - val_accuracy: 0.6451\n",
      "Epoch 11/300\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.9668 - accuracy: 0.6048 - val_loss: 0.8751 - val_accuracy: 0.6371\n",
      "Epoch 12/300\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.9478 - accuracy: 0.6160 - val_loss: 0.8558 - val_accuracy: 0.6576\n",
      "Epoch 13/300\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.9399 - accuracy: 0.6197 - val_loss: 0.8624 - val_accuracy: 0.6478\n",
      "Epoch 14/300\n",
      "141/141 [==============================] - 4s 25ms/step - loss: 0.9384 - accuracy: 0.6204 - val_loss: 0.8292 - val_accuracy: 0.6701\n",
      "Epoch 15/300\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.9114 - accuracy: 0.6364 - val_loss: 0.8342 - val_accuracy: 0.6679\n",
      "Epoch 16/300\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.9048 - accuracy: 0.6384 - val_loss: 0.8232 - val_accuracy: 0.6714\n",
      "Epoch 17/300\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 0.8846 - accuracy: 0.6475 - val_loss: 0.8113 - val_accuracy: 0.6705\n",
      "Epoch 18/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.8741 - accuracy: 0.6538 - val_loss: 0.8153 - val_accuracy: 0.6728\n",
      "Epoch 19/300\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 0.8705 - accuracy: 0.6620 - val_loss: 0.8013 - val_accuracy: 0.6817\n",
      "Epoch 20/300\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.8615 - accuracy: 0.6633 - val_loss: 0.8107 - val_accuracy: 0.6768\n",
      "Epoch 21/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.8576 - accuracy: 0.6681 - val_loss: 0.7904 - val_accuracy: 0.6915\n",
      "Epoch 22/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.8458 - accuracy: 0.6652 - val_loss: 0.7839 - val_accuracy: 0.6906\n",
      "Epoch 23/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.8411 - accuracy: 0.6741 - val_loss: 0.7849 - val_accuracy: 0.6866\n",
      "Epoch 24/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.8307 - accuracy: 0.6811 - val_loss: 0.7660 - val_accuracy: 0.6995\n",
      "Epoch 25/300\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.8352 - accuracy: 0.6830 - val_loss: 0.7572 - val_accuracy: 0.7057\n",
      "Epoch 26/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.8192 - accuracy: 0.6877 - val_loss: 0.7673 - val_accuracy: 0.7012\n",
      "Epoch 27/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.8164 - accuracy: 0.6844 - val_loss: 0.7626 - val_accuracy: 0.6919\n",
      "Epoch 28/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.8071 - accuracy: 0.6928 - val_loss: 0.7537 - val_accuracy: 0.7026\n",
      "Epoch 29/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.8047 - accuracy: 0.6926 - val_loss: 0.7551 - val_accuracy: 0.6999\n",
      "Epoch 30/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.7927 - accuracy: 0.7014 - val_loss: 0.7490 - val_accuracy: 0.7066\n",
      "Epoch 31/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.7926 - accuracy: 0.6986 - val_loss: 0.7443 - val_accuracy: 0.7115\n",
      "Epoch 32/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.7900 - accuracy: 0.6992 - val_loss: 0.7487 - val_accuracy: 0.6968\n",
      "Epoch 33/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.7804 - accuracy: 0.7071 - val_loss: 0.7298 - val_accuracy: 0.7164\n",
      "Epoch 34/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.7769 - accuracy: 0.7065 - val_loss: 0.7361 - val_accuracy: 0.7155\n",
      "Epoch 35/300\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 0.7800 - accuracy: 0.7025 - val_loss: 0.7271 - val_accuracy: 0.7195\n",
      "Epoch 36/300\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.7759 - accuracy: 0.7013 - val_loss: 0.7219 - val_accuracy: 0.7159\n",
      "Epoch 37/300\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.7662 - accuracy: 0.7144 - val_loss: 0.7169 - val_accuracy: 0.7275\n",
      "Epoch 38/300\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.7650 - accuracy: 0.7095 - val_loss: 0.7194 - val_accuracy: 0.7208\n",
      "Epoch 39/300\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.7599 - accuracy: 0.7208 - val_loss: 0.7211 - val_accuracy: 0.7222\n",
      "Epoch 40/300\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.7669 - accuracy: 0.7082 - val_loss: 0.7167 - val_accuracy: 0.7231\n",
      "Epoch 41/300\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.7538 - accuracy: 0.7181 - val_loss: 0.7242 - val_accuracy: 0.7164\n",
      "Epoch 42/300\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.7476 - accuracy: 0.7172 - val_loss: 0.7088 - val_accuracy: 0.7266\n",
      "Epoch 43/300\n",
      "141/141 [==============================] - 3s 24ms/step - loss: 0.7518 - accuracy: 0.7222 - val_loss: 0.7251 - val_accuracy: 0.7208\n",
      "Epoch 44/300\n",
      "141/141 [==============================] - 3s 23ms/step - loss: 0.7514 - accuracy: 0.7222 - val_loss: 0.7086 - val_accuracy: 0.7240\n",
      "Epoch 45/300\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 0.7425 - accuracy: 0.7196 - val_loss: 0.7095 - val_accuracy: 0.7293\n",
      "Epoch 46/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.7400 - accuracy: 0.7242 - val_loss: 0.7327 - val_accuracy: 0.7199\n",
      "Epoch 47/300\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.7431 - accuracy: 0.7246 - val_loss: 0.7150 - val_accuracy: 0.7213\n",
      "Epoch 48/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.7373 - accuracy: 0.7289 - val_loss: 0.7038 - val_accuracy: 0.7329\n",
      "Epoch 49/300\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 0.7254 - accuracy: 0.7334 - val_loss: 0.7002 - val_accuracy: 0.7320\n",
      "Epoch 50/300\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.7359 - accuracy: 0.7265 - val_loss: 0.7077 - val_accuracy: 0.7213\n",
      "Epoch 51/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.7295 - accuracy: 0.7273 - val_loss: 0.6946 - val_accuracy: 0.7373\n",
      "Epoch 52/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.7270 - accuracy: 0.7326 - val_loss: 0.7472 - val_accuracy: 0.7231\n",
      "Epoch 53/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.7305 - accuracy: 0.7269 - val_loss: 0.6948 - val_accuracy: 0.7342\n",
      "Epoch 54/300\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.7246 - accuracy: 0.7301 - val_loss: 0.6894 - val_accuracy: 0.7346\n",
      "Epoch 55/300\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.7189 - accuracy: 0.7378 - val_loss: 0.7179 - val_accuracy: 0.7235\n",
      "Epoch 56/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.7182 - accuracy: 0.7356 - val_loss: 0.6896 - val_accuracy: 0.7413\n",
      "Epoch 57/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.7071 - accuracy: 0.7396 - val_loss: 0.7110 - val_accuracy: 0.7271\n",
      "Epoch 58/300\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.7080 - accuracy: 0.7397 - val_loss: 0.6887 - val_accuracy: 0.7351\n",
      "Epoch 59/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.7120 - accuracy: 0.7355 - val_loss: 0.6992 - val_accuracy: 0.7329\n",
      "Epoch 60/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.7111 - accuracy: 0.7383 - val_loss: 0.6916 - val_accuracy: 0.7369\n",
      "Epoch 61/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.6974 - accuracy: 0.7424 - val_loss: 0.6910 - val_accuracy: 0.7400\n",
      "Epoch 62/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.7095 - accuracy: 0.7416 - val_loss: 0.6979 - val_accuracy: 0.7262\n",
      "Epoch 63/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.7060 - accuracy: 0.7364 - val_loss: 0.6860 - val_accuracy: 0.7373\n",
      "Epoch 64/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.7080 - accuracy: 0.7383 - val_loss: 0.6829 - val_accuracy: 0.7413\n",
      "Epoch 65/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.6916 - accuracy: 0.7476 - val_loss: 0.6835 - val_accuracy: 0.7382\n",
      "Epoch 66/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.6976 - accuracy: 0.7430 - val_loss: 0.6951 - val_accuracy: 0.7386\n",
      "Epoch 67/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.6996 - accuracy: 0.7422 - val_loss: 0.6988 - val_accuracy: 0.7280\n",
      "Epoch 68/300\n",
      "141/141 [==============================] - 2s 18ms/step - loss: 0.6926 - accuracy: 0.7443 - val_loss: 0.6824 - val_accuracy: 0.7413\n",
      "Epoch 69/300\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 0.7014 - accuracy: 0.7419 - val_loss: 0.7029 - val_accuracy: 0.7400\n",
      "Epoch 70/300\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.6938 - accuracy: 0.7437 - val_loss: 0.6911 - val_accuracy: 0.7400\n",
      "Epoch 71/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.6832 - accuracy: 0.7460 - val_loss: 0.6769 - val_accuracy: 0.7435\n",
      "Epoch 72/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.6892 - accuracy: 0.7458 - val_loss: 0.6868 - val_accuracy: 0.7382\n",
      "Epoch 73/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.6859 - accuracy: 0.7482 - val_loss: 0.6874 - val_accuracy: 0.7404\n",
      "Epoch 74/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.6900 - accuracy: 0.7491 - val_loss: 0.6793 - val_accuracy: 0.7435\n",
      "Epoch 75/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.6910 - accuracy: 0.7465 - val_loss: 0.6753 - val_accuracy: 0.7476\n",
      "Epoch 76/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.6837 - accuracy: 0.7492 - val_loss: 0.6832 - val_accuracy: 0.7431\n",
      "Epoch 77/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.6847 - accuracy: 0.7507 - val_loss: 0.7134 - val_accuracy: 0.7297\n",
      "Epoch 78/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.6889 - accuracy: 0.7466 - val_loss: 0.6729 - val_accuracy: 0.7431\n",
      "Epoch 79/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.6812 - accuracy: 0.7501 - val_loss: 0.6784 - val_accuracy: 0.7422\n",
      "Epoch 80/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.6847 - accuracy: 0.7521 - val_loss: 0.6745 - val_accuracy: 0.7458\n",
      "Epoch 81/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.6760 - accuracy: 0.7506 - val_loss: 0.6775 - val_accuracy: 0.7440\n",
      "Epoch 82/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.6734 - accuracy: 0.7542 - val_loss: 0.6723 - val_accuracy: 0.7440\n",
      "Epoch 83/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.6683 - accuracy: 0.7514 - val_loss: 0.6789 - val_accuracy: 0.7409\n",
      "Epoch 84/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.6764 - accuracy: 0.7523 - val_loss: 0.6799 - val_accuracy: 0.7476\n",
      "Epoch 85/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.6788 - accuracy: 0.7491 - val_loss: 0.6884 - val_accuracy: 0.7435\n",
      "Epoch 86/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.6777 - accuracy: 0.7486 - val_loss: 0.7301 - val_accuracy: 0.7226\n",
      "Epoch 87/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.6650 - accuracy: 0.7502 - val_loss: 0.6840 - val_accuracy: 0.7391\n",
      "Epoch 88/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.6645 - accuracy: 0.7547 - val_loss: 0.6648 - val_accuracy: 0.7480\n",
      "Epoch 89/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.6676 - accuracy: 0.7567 - val_loss: 0.6732 - val_accuracy: 0.7471\n",
      "Epoch 90/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.6726 - accuracy: 0.7533 - val_loss: 0.6748 - val_accuracy: 0.7484\n",
      "Epoch 91/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.6593 - accuracy: 0.7615 - val_loss: 0.6903 - val_accuracy: 0.7369\n",
      "Epoch 92/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.6608 - accuracy: 0.7542 - val_loss: 0.6889 - val_accuracy: 0.7444\n",
      "Epoch 93/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.6742 - accuracy: 0.7582 - val_loss: 0.6854 - val_accuracy: 0.7462\n",
      "Epoch 94/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.6617 - accuracy: 0.7597 - val_loss: 0.6729 - val_accuracy: 0.7435\n",
      "Epoch 95/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.6564 - accuracy: 0.7583 - val_loss: 0.6971 - val_accuracy: 0.7382\n",
      "Epoch 96/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.6576 - accuracy: 0.7599 - val_loss: 0.6704 - val_accuracy: 0.7551\n",
      "Epoch 97/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.6539 - accuracy: 0.7580 - val_loss: 0.7689 - val_accuracy: 0.7115\n",
      "Epoch 98/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.6567 - accuracy: 0.7594 - val_loss: 0.6694 - val_accuracy: 0.7498\n",
      "Epoch 99/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.6609 - accuracy: 0.7585 - val_loss: 0.6803 - val_accuracy: 0.7480\n",
      "Epoch 100/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.6599 - accuracy: 0.7590 - val_loss: 0.6730 - val_accuracy: 0.7529\n",
      "Epoch 101/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.6620 - accuracy: 0.7525 - val_loss: 0.6695 - val_accuracy: 0.7524\n",
      "Epoch 102/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.6549 - accuracy: 0.7610 - val_loss: 0.6774 - val_accuracy: 0.7458\n",
      "Epoch 103/300\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.6484 - accuracy: 0.7574 - val_loss: 0.6752 - val_accuracy: 0.7413\n",
      "Epoch 104/300\n",
      "141/141 [==============================] - 2s 17ms/step - loss: 0.6530 - accuracy: 0.7630 - val_loss: 0.6755 - val_accuracy: 0.7484\n",
      "Epoch 105/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.6544 - accuracy: 0.7621 - val_loss: 0.6745 - val_accuracy: 0.7462\n",
      "Epoch 106/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.6600 - accuracy: 0.7556 - val_loss: 0.7015 - val_accuracy: 0.7431\n",
      "Epoch 107/300\n",
      "141/141 [==============================] - 2s 15ms/step - loss: 0.6473 - accuracy: 0.7625 - val_loss: 0.6917 - val_accuracy: 0.7435\n",
      "Epoch 108/300\n",
      "141/141 [==============================] - 2s 16ms/step - loss: 0.6424 - accuracy: 0.7654 - val_loss: 0.6869 - val_accuracy: 0.7422\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "# 모델 정의\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=500, output_dim=128, input_length=7))\n",
    "model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(4, activation='softmax')) # 클래스의 개수에 맞게 설정\n",
    "\n",
    "\n",
    "initial_learning_rate = 0.005\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps=10000, decay_rate=0.9)\n",
    "\n",
    "optimizer = SGD(learning_rate=lr_schedule)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 콜백 정의\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-embedding-model.h5', save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(train_input, train_target, epochs=300, batch_size=64,\n",
    "                    validation_data=(val_input, val_target),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s 3ms/step - loss: 0.6898 - accuracy: 0.7432\n",
      "Test Loss: 0.6898108720779419\n",
      "Test Accuracy: 0.7432336211204529\n"
     ]
    }
   ],
   "source": [
    "# 패딩이 완료된 테스트 데이터\n",
    "test_input = pad_sequences(test_input, maxlen=8)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_input, test_target)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3. 문장에서 감정분석 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "입력 문장: 친구랑 싸워서 기분이 너무 안좋아\n",
      "예측된 감정: 슬픔\n",
      "각 클래스의 확률: [0.41163877 0.4180796  0.12715481 0.04312674]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "# Assuming 'model' and 'word_to_index' are defined before this code\n",
    "\n",
    "# Load the Kkma tokenizer\n",
    "kkma = Kkma()\n",
    "\n",
    "def tokenize_and_pad(sentence, tokenizer, word_to_index, max_length=7):\n",
    "    input_tokens = [word for word, pos in tokenizer.pos(sentence) if (pos == 'NNG') or (pos == 'VV') or (pos == 'NP') or (pos == 'VA') or (pos == 'MAG')]\n",
    "    input_indices = [word_to_index.get(word, 0) for word in input_tokens]\n",
    "    padded_input = pad_sequences([input_indices], maxlen=max_length)\n",
    "    return padded_input\n",
    "\n",
    "# 사용 예시\n",
    "while True:\n",
    "    input_sentence = input()\n",
    "    if input_sentence == '0':\n",
    "        break\n",
    "\n",
    "    padded_input = tokenize_and_pad(input_sentence, kkma, word_to_index)\n",
    "\n",
    "    predicted_probabilities = model.predict(padded_input)\n",
    "    predicted_class = np.argmax(predicted_probabilities)\n",
    "\n",
    "    emotions = ['분노', '슬픔', '불안', '기쁨']\n",
    "    predicted_emotion = emotions[predicted_class]\n",
    "\n",
    "    print(f\"입력 문장: {input_sentence}\")\n",
    "    print(f\"예측된 감정: {predicted_emotion}\")\n",
    "    print(f\"각 클래스의 확률: {predicted_probabilities[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\kikikiju\\Desktop\\saved_model\\12-10-model.pt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\kikikiju\\Desktop\\saved_model\\12-10-model.pt\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('C:\\\\Users\\\\kikikiju\\\\Desktop\\\\saved_model\\\\12-10-model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plus. 모델 로드 후 감정분석을 통해 음악 플레이리스트 추천해주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model2 = tf.keras.models.load_model('C:\\\\Users\\\\kikikiju\\\\Desktop\\\\saved_model\\\\12-10-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kikikiju\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word_to_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m padded_input\n\u001b[0;32m     16\u001b[0m input_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m()\n\u001b[1;32m---> 18\u001b[0m padded_input \u001b[38;5;241m=\u001b[39m tokenize_and_pad(input_sentence, kkma, \u001b[43mword_to_index\u001b[49m)\n\u001b[0;32m     19\u001b[0m predicted_probabilities \u001b[38;5;241m=\u001b[39m model2\u001b[38;5;241m.\u001b[39mpredict(padded_input)\n\u001b[0;32m     20\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predicted_probabilities)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_to_index' is not defined"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "\n",
    "# Load the Kkma tokenizer\n",
    "kkma = Kkma()\n",
    "\n",
    "def tokenize_and_pad(sentence, tokenizer, word_to_index, max_length=7):\n",
    "    input_tokens = [word for word, pos in tokenizer.pos(sentence) if (pos == 'NNG') or (pos == 'VV') or (pos == 'NP') or (pos == 'VA') or (pos == 'MAG')]\n",
    "    input_indices = [word_to_index.get(word, 0) for word in input_tokens]\n",
    "    padded_input = pad_sequences([input_indices], maxlen=max_length)\n",
    "    return padded_input\n",
    "\n",
    "input_sentence = input()\n",
    "\n",
    "padded_input = tokenize_and_pad(input_sentence, kkma, word_to_index)\n",
    "predicted_probabilities = model2.predict(padded_input)\n",
    "predicted_class = np.argmax(predicted_probabilities)\n",
    "\n",
    "emotions = ['분노', '슬픔', '불안', '기쁨']\n",
    "predicted_emotion = emotions[predicted_class]\n",
    "\n",
    "print(f\"입력 문장: {input_sentence}\")\n",
    "print(f\"예측된 감정: {predicted_emotion}\")\n",
    "print(f\"각 클래스의 확률: {predicted_probabilities[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분노\n",
      "화날 때 듣기 좋은 음악\n",
      "\n",
      "상위 5개 동영상 주소:\n",
      "1. https://www.youtube.com/watch?v=6atvmgoKKNQ\n",
      "2. https://www.youtube.com/watch?v=71hZutqP_cM\n",
      "3. https://www.youtube.com/watch?v=8yC_fnORfwU\n",
      "4. https://www.youtube.com/watch?v=mEKAXv6gSgw\n",
      "5. https://www.youtube.com/watch?v=RtPwBk0pqKE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "def search_youtube(api_key, query, max_results=5):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # 검색 실행\n",
    "    search_response = youtube.search().list(\n",
    "        q=query,\n",
    "        type='video',\n",
    "        part='id,snippet',\n",
    "        maxResults=max_results\n",
    "    ).execute()\n",
    "\n",
    "    # 결과에서 동영상의 ID 추출\n",
    "    video_ids = [item['id']['videoId'] for item in search_response['items']]\n",
    "\n",
    "    # 각 동영상의 주소 생성\n",
    "    video_urls = [f'https://www.youtube.com/watch?v={video_id}' for video_id in video_ids]\n",
    "\n",
    "    return video_urls\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # YouTube API 키를 설정\n",
    "    youtube_api_key = \"\"\n",
    "\n",
    "    # 검색어를 입력\n",
    "    if(predicted_emotion == \"분노\"):\n",
    "        print(predicted_emotion)\n",
    "        search_query = \"화날 때 듣기 좋은 음악\"\n",
    "    elif(predicted_emotion == \"슬픔\"):\n",
    "        print(predicted_emotion)\n",
    "        search_query = \"슬플 때 듣기 좋은 음악\"\n",
    "    elif(predicted_emotion == \"불안\"):\n",
    "        print(predicted_emotion)        \n",
    "        search_query =  \"불안할 때 듣기 좋은 음악\"\n",
    "    elif(predicted_emotion == \"기쁨\"):\n",
    "        print(predicted_emotion)\n",
    "        search_query = \"기쁠 때 듣기 좋은 음악\"\n",
    "    \n",
    "    print(search_query)\n",
    "\n",
    "    # YouTube API를 사용하여 동영상 주소 가져오기\n",
    "    video_urls = search_youtube(youtube_api_key, search_query)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"\\n상위 5개 동영상 주소:\")\n",
    "    for i, video_url in enumerate(video_urls, start=1):\n",
    "        print(f\"{i}. {video_url}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
