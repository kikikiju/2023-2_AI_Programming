{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 감정분석 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. 데이터 수집\n",
    "## 1-1. csv파일로 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문장</th>\n",
       "      <th>감정</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>내가 좋아하는 유튜버가 좋은 제품이라고 하길래 나도 샀지. 그런데 그게 돈 받고 하...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그래서 사람들이 증거 사진을 올렸더니 그 때서야 광고였다고 미안하다고 하는 거야.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>처음부터 자기 잘못을 알고 미안하다고 했으면 화가 이렇게 나진 않았을 거야. 사람들...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>내 휴가가 엉망진창이 돼버렸어.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>휴가 시작함과 동시에 장마도 같이 시작해버렸어. 그래서 하루 종일 비가 와.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52112</th>\n",
       "      <td>입사 초반에는 인간관계가 힘들었는데 이제 좀 편안해진 것 같아.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52113</th>\n",
       "      <td>빨리 결혼식을 하고 싶어.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52114</th>\n",
       "      <td>이번에 무사히 출산했어. 정말 다행이야.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52115</th>\n",
       "      <td>아이가 뛰어놀다가 넘어져서 깜짝 놀랐었어.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52116</th>\n",
       "      <td>남자친구랑 헤어질 뻔했는데 다시 화해했어. 정말 다행이야.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52117 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      문장  감정\n",
       "0      내가 좋아하는 유튜버가 좋은 제품이라고 하길래 나도 샀지. 그런데 그게 돈 받고 하...   0\n",
       "1          그래서 사람들이 증거 사진을 올렸더니 그 때서야 광고였다고 미안하다고 하는 거야.   0\n",
       "2      처음부터 자기 잘못을 알고 미안하다고 했으면 화가 이렇게 나진 않았을 거야. 사람들...   0\n",
       "3                                      내 휴가가 엉망진창이 돼버렸어.   0\n",
       "4             휴가 시작함과 동시에 장마도 같이 시작해버렸어. 그래서 하루 종일 비가 와.   0\n",
       "...                                                  ...  ..\n",
       "52112                입사 초반에는 인간관계가 힘들었는데 이제 좀 편안해진 것 같아.   3\n",
       "52113                                     빨리 결혼식을 하고 싶어.   3\n",
       "52114                             이번에 무사히 출산했어. 정말 다행이야.   3\n",
       "52115                            아이가 뛰어놀다가 넘어져서 깜짝 놀랐었어.   3\n",
       "52116                   남자친구랑 헤어질 뻔했는데 다시 화해했어. 정말 다행이야.   3\n",
       "\n",
       "[52117 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data_file.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감정\n",
      "0    13680\n",
      "2    13574\n",
      "1    12976\n",
      "3    11887\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_count = df['감정'].value_counts()\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "target_list = []\n",
    "\n",
    "for q,  label in zip(df['문장'], df['감정']):\n",
    "    data_list.append(q)\n",
    "    target_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "\n",
    "kkma = Kkma()\n",
    "\n",
    "tokenized_data = []\n",
    "\n",
    "for s in data_list:\n",
    "    text_2 = [word for word, pos in kkma.pos(s) if (pos == 'NNG') or (pos == 'VV') or (pos == 'NP') or (pos == 'VA') or (pos == 'MAG')] # 조사 또는 문장부호는 빼줌\n",
    "    #if text_2:  # 빈 리스트가 아닌 경우에만 추가\n",
    "    tokenized_data.append(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('나', 'NP'), ('가', 'JKS'), ('좋아하', 'VV'), ('는', 'ETD'), ('유튜버', 'UN'), ('가', 'JKS'), ('좋', 'VA'), ('은', 'ETD'), ('제품', 'NNG'), ('이', 'VCP'), ('라고', 'ECD'), ('하', 'VV'), ('길래', 'ECD'), ('나도', 'NNG'), ('사', 'VV'), ('었', 'EPT'), ('지', 'EFN'), ('.', 'SF'), ('그러', 'VV'), ('ㄴ', 'ETD'), ('데', 'NNB'), ('그', 'VA'), ('게', 'ECD'), ('돈', 'NNG'), ('받', 'VV'), ('고', 'ECE'), ('하', 'VV'), ('는', 'ETD'), ('광고였대잖', 'UN'), ('아', 'JKI'), ('.', 'SF')]\n",
      "내가 좋아하는 유튜버가 좋은 제품이라고 하길래 나도 샀지. 그런데 그게 돈 받고 하는 광고였대잖아.\n",
      "['나', '좋아하', '좋', '제품', '하', '나도', '사', '그러', '그', '돈', '받', '하']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(kkma.pos(data_list[0]))\n",
    "print(data_list[0])\n",
    "print(tokenized_data[0])\n",
    "print(target_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 전체 데이터에 대한 단어 빈도수 계산\n",
    "word_counts = Counter(word for sentence in tokenized_data for word in sentence)\n",
    "\n",
    "# 빈도수에 따라 단어를 정렬\n",
    "sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 상위 N개의 단어로 단어 사전 만들기\n",
    "top_words = [word for word, count in sorted_words[:500]]\n",
    "word_to_index = {word: idx for idx, word in enumerate(top_words)}\n",
    "\n",
    "# 모든 데이터를 숫자로 변환\n",
    "num_tokenized_data = [[word_to_index.get(word, 0) for word in sentence] for sentence in tokenized_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'하': 0, '나': 1, '너무': 2, '친구': 3, '같': 4, '되': 5, '있': 6, '없': 7, '내': 8, '안': 9, '좋': 10, '보': 11, '일': 12, '내가': 13, '요즘': 14, '사람': 15, '힘들': 16, '정말': 17, '받': 18, '듣': 19, '가': 20, '집': 21, '슬프': 22, '오늘': 23, '못': 24, '회사': 25, '알': 26, '걱정': 27, '생각': 28, '다': 29, '많이': 30, '돈': 31, '기분': 32, '아내': 33, '오': 34, '남편': 35, '때': 36, '이번': 37, '잘': 38, '우리': 39, '모르': 40, '말': 41, '마음': 42, '건강': 43, '부모님': 44, '이제': 45, '살': 46, '결혼': 47, '많': 48, '지금': 49, '나이': 50, '불안': 51, '학교': 52, '엄마': 53, '더': 54, '아프': 55, '몸': 56, '왜': 57, '나오': 58, '그러': 59, '자꾸': 60, '직장': 61, '먹': 62, '화': 63, '짜증': 64, '가족': 65, '좀': 66, '생기': 67, '기쁘': 68, '준비': 69, '만나': 70, '아들': 71, '자식': 72, '계속': 73, '어떻': 74, '취업': 75, '당하': 76, '그리하': 77, '낳': 78, '스트레스': 79, '싫': 80, '화가': 81, '공부': 82, '아이': 83, '죽': 84, '사': 85, '날': 86, '주식': 87, '크': 88, '혼자': 89, '갑자기': 90, '아직': 91, '나가': 92, '그렇': 93, '들': 94, '떨어지': 95, '남자': 96, '속상하': 97, '성적': 98, '아빠': 99, '애': 100, '같이': 101, '전': 102, '무섭': 103, '맞': 104, '우울': 105, '드디어': 106, '끝나': 107, '곰팡이': 108, '딸': 109, '싸우': 110, '다니': 111, '노후': 112, '쓰': 113, '진짜': 114, '해피': 115, '당첨': 116, '병원': 117, '어렵': 118, '어제': 119, '시험': 120, '자신': 121, '뭐': 122, '놀라': 123, '여자': 124, '은퇴': 125, '갈': 126, '주': 127, '자': 128, '좋아하': 129, '프로젝트': 130, '열심히': 131, '괜찮': 132, '괴롭히': 133, '코로나': 134, '시간': 135, '이벤트': 136, '대하': 137, '엄청': 138, '음악': 139, '해지': 140, '실망': 141, '심하': 142, '고맙': 143, '다행': 144, '대학': 145, '남': 146, '상사': 147, '주변': 148, '연락': 149, '갇히': 150, '후회': 151, '선생님': 152, '눈물': 153, '걸리': 154, '짜증나': 155, '고민': 156, '어': 157, '일하': 158, '얼마': 159, '시작하': 160, '아': 161, '문제': 162, '업무': 163, '나르': 164, '여행': 165, '친하': 166, '항상': 167, '또': 168, '앞': 169, '않': 170, '매일': 171, '후': 172, '청소': 173, '두렵': 174, '이혼': 175, '운동': 176, '생활': 177, '말하': 178, '참': 179, '그냥': 180, '잘하': 181, '휴가': 182, '추천': 183, '화장실': 184, '치': 185, '좋아지': 186, '믿': 187, '밖': 188, '동안': 189, '얘기': 190, '그': 191, '없이': 192, '놀': 193, '그래서': 194, '보이': 195, '우울하': 196, '테': 197, '이렇': 198, '점점': 199, '신나': 200, '반': 201, '가지': 202, '면접': 203, '벌': 204, '무엇': 205, '관계': 206, '검진': 207, '달': 208, '다행히': 209, '조금': 210, '들어오': 211, '나한': 212, '산책': 213, '이상': 214, '축하': 215, '빨리': 216, '걸': 217, '깜짝': 218, '소리': 219, '아주': 220, '폭력': 221, '세상': 222, '잠': 223, '마라톤': 224, '선물': 225, '곳': 226, '아까': 227, '모두': 228, '술': 229, '직원': 230, '아니': 231, '함께': 232, '따돌림': 233, '위하': 234, '행복': 235, '모으': 236, '있었': 237, '진로': 238, '조심': 239, '쉬': 240, '신경': 241, '전화': 242, '초조': 243, '힘': 244, '결과': 245, '무시': 246, '내일': 247, '기록': 248, '지내': 249, '나도': 250, '원하': 251, '장애': 252, '대회': 253, '불편': 254, '병': 255, '혼란': 256, '요새': 257, '암': 258, '들어가': 259, '동료': 260, '상황': 261, '귀찮': 262, '동생': 263, '이야기': 264, '새끼': 265, '시키': 266, '느껴지': 267, '느끼': 268, '연애': 269, '아무': 270, '오르': 271, '예전': 272, '자기': 273, '물': 274, '자주': 275, '이러': 276, '우울증': 277, '늙': 278, '영화': 279, '향수': 280, '사실': 281, '목줄': 282, '감사': 283, '퇴직': 284, '처음': 285, '떠나': 286, '수술': 287, '합격': 288, '곧': 289, '비': 290, '의사': 291, '필요': 292, '찾': 293, '약속': 294, '젊': 295, '지치': 296, '오랜만': 297, '시작': 298, '대화': 299, '없었': 300, '꼭': 301, '그만두': 302, '뽑': 303, '사이': 304, '치료': 305, '얼마나': 306, '지인': 307, '까': 308, '울': 309, '방': 310, '잘못': 311, '헐': 312, '바쁘': 313, '고생': 314, '오래': 315, '구매': 316, '너': 317, '이렇게': 318, '나쁘': 319, '늦': 320, '다시': 321, '아무것': 322, '왕따': 323, '늘': 324, '사과': 325, '막': 326, '잡': 327, '노트북': 328, '이해': 329, '보내': 330, '완전히': 331, '최근': 332, '고': 333, '언제': 334, '효과': 335, '해보': 336, '승진': 337, '어리': 338, '질환': 339, '따돌리': 340, '달라': 341, '갖': 342, '달리': 343, '하루': 344, '실수': 345, '사고': 346, '인생': 347, '다치': 348, '성가시': 349, '진정': 350, '언니': 351, '드': 352, '길': 353, '그것': 354, '치매': 355, '정도': 356, '이유': 357, '놀래': 358, '할머니': 359, '어떡하': 360, '벌써': 361, '챙기': 362, '완전': 363, '병원비': 364, '그동안': 365, '다리': 366, '편하': 367, '당황': 368, '취직': 369, '꿈': 370, '별로': 371, '도와주': 372, '다투': 373, '다르': 374, '욕': 375, '노력': 376, '빌리': 377, '안되': 378, '제거제': 379, '뛰': 380, '노인': 381, '약': 382, '다녀오': 383, '가기': 384, '구들': 385, '비싸': 386, '당혹': 387, '모습': 388, '끊어지': 389, '몰르': 390, '닿': 391, '아무리': 392, '차': 393, '무기력': 394, '경제적': 395, '너무나': 396, '계획': 397, '도움': 398, '기다리': 399, '편안': 400, '스': 401, '제대로': 402, '밥': 403, '보험': 404, '근데': 405, '옆': 406, '출산': 407, '느낌': 408, '부족': 409, '발표': 410, '맨날': 411, '덜': 412, '삶': 413, '헤어지': 414, '생일': 415, '관리': 416, '만족': 417, '미안': 418, '사라지': 419, '지원': 420, '사귀': 421, '사업': 422, '쉽': 423, '줄': 424, '다음': 425, '모임': 426, '서럽': 427, '잔소리': 428, '먼저': 429, '제일': 430, '자녀': 431, '아무래도': 432, '빠지': 433, '용돈': 434, '자랑': 435, '당뇨': 436, '투자': 437, '키우': 438, '돌보': 439, '잃': 440, '나랑': 441, '새': 442, '어디': 443, '천둥': 444, '구역질': 445, '임신': 446, '어쩌': 447, '김': 448, '새로': 449, '관심': 450, '하나': 451, '매번': 452, '뒤': 453, '상태': 454, '매우': 455, '출근': 456, '낮': 457, '물어보': 458, '같애': 459, '눈': 460, '넘': 461, '느': 462, '팔': 463, '졸업': 464, '입원': 465, '옆집': 466, '사랑': 467, '집중': 468, '갱신': 469, '점수': 470, '아버지': 471, '머리': 472, '거지': 473, '열': 474, '동네': 475, '팀장': 476, '마비': 477, '인하': 478, '눈치': 479, '며칠': 480, '손해': 481, '풀': 482, '멀': 483, '적': 484, '소식': 485, '감정': 486, '타': 487, '직업': 488, '평소': 489, '어머니': 490, '대학교': 491, '당연히': 492, '끝내': 493, '영상': 494, '만들': 495, '때리': 496, '정신': 497, '긴장': 498, '한번': 499}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하: 15767번\n",
      "나: 15443번\n",
      "너무: 9449번\n",
      "친구: 6133번\n",
      "같: 5471번\n",
      "되: 5365번\n",
      "있: 4958번\n",
      "없: 4853번\n",
      "내: 4535번\n",
      "안: 4260번\n",
      "좋: 4231번\n",
      "보: 3817번\n",
      "일: 2944번\n",
      "내가: 2883번\n",
      "요즘: 2579번\n",
      "사람: 2498번\n",
      "힘들: 2432번\n",
      "정말: 2366번\n",
      "받: 2337번\n",
      "듣: 2325번\n",
      "가: 2227번\n",
      "집: 2041번\n",
      "슬프: 2023번\n",
      "오늘: 2016번\n",
      "못: 2008번\n",
      "회사: 1972번\n",
      "알: 1924번\n",
      "걱정: 1913번\n",
      "생각: 1819번\n",
      "다: 1733번\n",
      "많이: 1715번\n",
      "돈: 1694번\n",
      "기분: 1662번\n",
      "아내: 1662번\n",
      "오: 1628번\n",
      "남편: 1627번\n",
      "때: 1623번\n",
      "이번: 1610번\n",
      "잘: 1604번\n",
      "우리: 1501번\n",
      "모르: 1475번\n",
      "말: 1470번\n",
      "마음: 1437번\n",
      "건강: 1399번\n",
      "부모님: 1389번\n",
      "이제: 1307번\n",
      "살: 1303번\n",
      "결혼: 1301번\n",
      "많: 1294번\n",
      "지금: 1274번\n",
      "나이: 1255번\n",
      "불안: 1239번\n",
      "학교: 1213번\n",
      "엄마: 1185번\n",
      "더: 1138번\n",
      "아프: 1128번\n",
      "몸: 1118번\n",
      "왜: 1107번\n",
      "나오: 1104번\n",
      "그러: 1092번\n",
      "자꾸: 1062번\n",
      "직장: 1062번\n",
      "먹: 1030번\n",
      "화: 1011번\n",
      "짜증: 989번\n",
      "가족: 967번\n",
      "좀: 959번\n",
      "생기: 958번\n",
      "기쁘: 952번\n",
      "준비: 948번\n",
      "만나: 939번\n",
      "아들: 914번\n",
      "자식: 903번\n",
      "계속: 899번\n",
      "어떻: 892번\n",
      "취업: 889번\n",
      "당하: 887번\n",
      "그리하: 881번\n",
      "낳: 864번\n",
      "스트레스: 858번\n",
      "싫: 852번\n",
      "화가: 842번\n",
      "공부: 819번\n",
      "아이: 815번\n",
      "죽: 806번\n",
      "사: 785번\n",
      "날: 769번\n",
      "주식: 769번\n",
      "크: 764번\n",
      "혼자: 764번\n",
      "갑자기: 761번\n",
      "아직: 756번\n",
      "나가: 753번\n",
      "그렇: 752번\n",
      "들: 750번\n",
      "떨어지: 749번\n",
      "남자: 745번\n",
      "속상하: 737번\n",
      "성적: 736번\n",
      "아빠: 735번\n",
      "애: 726번\n",
      "같이: 719번\n",
      "전: 710번\n",
      "무섭: 705번\n",
      "맞: 702번\n",
      "우울: 691번\n",
      "드디어: 688번\n",
      "끝나: 683번\n",
      "곰팡이: 676번\n",
      "딸: 675번\n",
      "싸우: 664번\n",
      "다니: 663번\n",
      "노후: 663번\n",
      "쓰: 659번\n",
      "진짜: 655번\n",
      "해피: 655번\n",
      "당첨: 655번\n",
      "병원: 646번\n",
      "어렵: 643번\n",
      "어제: 642번\n",
      "시험: 630번\n",
      "자신: 619번\n",
      "뭐: 614번\n",
      "놀라: 611번\n",
      "여자: 597번\n",
      "은퇴: 596번\n",
      "갈: 594번\n",
      "주: 593번\n",
      "자: 588번\n",
      "좋아하: 577번\n",
      "프로젝트: 576번\n",
      "열심히: 576번\n",
      "괜찮: 572번\n",
      "괴롭히: 561번\n",
      "코로나: 555번\n",
      "시간: 550번\n",
      "이벤트: 545번\n",
      "대하: 537번\n",
      "엄청: 525번\n",
      "음악: 518번\n",
      "해지: 509번\n",
      "실망: 507번\n",
      "심하: 507번\n",
      "고맙: 504번\n",
      "다행: 499번\n",
      "대학: 497번\n",
      "남: 496번\n",
      "상사: 494번\n",
      "주변: 492번\n",
      "연락: 489번\n",
      "갇히: 489번\n",
      "후회: 485번\n",
      "선생님: 484번\n",
      "눈물: 481번\n",
      "걸리: 480번\n",
      "짜증나: 478번\n",
      "고민: 478번\n",
      "어: 477번\n",
      "일하: 474번\n",
      "얼마: 472번\n",
      "시작하: 471번\n",
      "아: 471번\n",
      "문제: 470번\n",
      "업무: 466번\n",
      "나르: 466번\n",
      "여행: 464번\n",
      "친하: 464번\n",
      "항상: 458번\n",
      "또: 457번\n",
      "앞: 453번\n",
      "않: 448번\n",
      "매일: 447번\n",
      "후: 441번\n",
      "청소: 440번\n",
      "두렵: 433번\n",
      "이혼: 432번\n",
      "운동: 427번\n",
      "생활: 421번\n",
      "말하: 417번\n",
      "참: 413번\n",
      "그냥: 412번\n",
      "잘하: 411번\n",
      "휴가: 397번\n",
      "추천: 397번\n",
      "화장실: 394번\n",
      "치: 393번\n",
      "좋아지: 391번\n",
      "믿: 390번\n",
      "밖: 390번\n",
      "동안: 388번\n",
      "얘기: 388번\n",
      "그: 386번\n",
      "없이: 385번\n",
      "놀: 383번\n",
      "그래서: 382번\n",
      "보이: 382번\n",
      "우울하: 382번\n",
      "테: 381번\n",
      "이렇: 379번\n",
      "점점: 374번\n",
      "신나: 373번\n",
      "반: 373번\n",
      "가지: 369번\n",
      "면접: 368번\n",
      "벌: 365번\n",
      "무엇: 362번\n",
      "관계: 359번\n",
      "검진: 359번\n",
      "달: 357번\n",
      "다행히: 354번\n",
      "조금: 350번\n",
      "들어오: 349번\n",
      "나한: 348번\n",
      "산책: 347번\n",
      "이상: 346번\n",
      "축하: 343번\n",
      "빨리: 342번\n",
      "걸: 340번\n",
      "깜짝: 336번\n",
      "소리: 335번\n",
      "아주: 331번\n",
      "폭력: 330번\n",
      "세상: 328번\n",
      "잠: 327번\n",
      "마라톤: 327번\n",
      "선물: 326번\n",
      "곳: 324번\n",
      "아까: 324번\n",
      "모두: 323번\n",
      "술: 323번\n",
      "직원: 322번\n",
      "아니: 320번\n",
      "함께: 320번\n",
      "따돌림: 320번\n",
      "위하: 319번\n",
      "행복: 317번\n",
      "모으: 315번\n",
      "있었: 312번\n",
      "진로: 312번\n",
      "조심: 311번\n",
      "쉬: 310번\n",
      "신경: 308번\n",
      "전화: 306번\n",
      "초조: 306번\n",
      "힘: 305번\n",
      "결과: 304번\n",
      "무시: 303번\n",
      "내일: 300번\n",
      "기록: 300번\n",
      "지내: 293번\n",
      "나도: 290번\n",
      "원하: 290번\n",
      "장애: 287번\n",
      "대회: 286번\n",
      "불편: 285번\n",
      "병: 283번\n",
      "혼란: 283번\n",
      "요새: 281번\n",
      "암: 281번\n",
      "들어가: 280번\n",
      "동료: 280번\n",
      "상황: 280번\n",
      "귀찮: 280번\n",
      "동생: 280번\n",
      "이야기: 278번\n",
      "새끼: 277번\n",
      "시키: 275번\n",
      "느껴지: 273번\n",
      "느끼: 272번\n",
      "연애: 269번\n",
      "아무: 268번\n",
      "오르: 268번\n",
      "예전: 267번\n",
      "자기: 265번\n",
      "물: 264번\n",
      "자주: 263번\n",
      "이러: 262번\n",
      "우울증: 262번\n",
      "늙: 261번\n",
      "영화: 259번\n",
      "향수: 256번\n",
      "사실: 254번\n",
      "목줄: 254번\n",
      "감사: 252번\n",
      "퇴직: 252번\n",
      "처음: 251번\n",
      "떠나: 250번\n",
      "수술: 250번\n",
      "합격: 250번\n",
      "곧: 248번\n",
      "비: 247번\n",
      "의사: 247번\n",
      "필요: 246번\n",
      "찾: 245번\n",
      "약속: 244번\n",
      "젊: 244번\n",
      "지치: 243번\n",
      "오랜만: 242번\n",
      "시작: 241번\n",
      "대화: 241번\n",
      "없었: 240번\n",
      "꼭: 240번\n",
      "그만두: 239번\n",
      "뽑: 239번\n",
      "사이: 238번\n",
      "치료: 236번\n",
      "얼마나: 235번\n",
      "지인: 235번\n",
      "까: 234번\n",
      "울: 234번\n",
      "방: 234번\n",
      "잘못: 233번\n",
      "헐: 233번\n",
      "바쁘: 232번\n",
      "고생: 232번\n",
      "오래: 230번\n",
      "구매: 229번\n",
      "너: 229번\n",
      "이렇게: 228번\n",
      "나쁘: 228번\n",
      "늦: 228번\n",
      "다시: 228번\n",
      "아무것: 228번\n",
      "왕따: 228번\n",
      "늘: 227번\n",
      "사과: 226번\n",
      "막: 226번\n",
      "잡: 225번\n",
      "노트북: 225번\n",
      "이해: 224번\n",
      "보내: 224번\n",
      "완전히: 224번\n",
      "최근: 224번\n",
      "고: 223번\n",
      "언제: 223번\n",
      "효과: 222번\n",
      "해보: 221번\n",
      "승진: 220번\n",
      "어리: 218번\n",
      "질환: 218번\n",
      "따돌리: 217번\n",
      "달라: 216번\n",
      "갖: 214번\n",
      "달리: 214번\n",
      "하루: 213번\n",
      "실수: 213번\n",
      "사고: 212번\n",
      "인생: 212번\n",
      "다치: 212번\n",
      "성가시: 212번\n",
      "진정: 210번\n",
      "언니: 210번\n",
      "드: 209번\n",
      "길: 208번\n",
      "그것: 207번\n",
      "치매: 206번\n",
      "정도: 205번\n",
      "이유: 204번\n",
      "놀래: 203번\n",
      "할머니: 203번\n",
      "어떡하: 202번\n",
      "벌써: 202번\n",
      "챙기: 201번\n",
      "완전: 200번\n",
      "병원비: 200번\n",
      "그동안: 199번\n",
      "다리: 199번\n",
      "편하: 198번\n",
      "당황: 196번\n",
      "취직: 196번\n",
      "꿈: 195번\n",
      "별로: 194번\n",
      "도와주: 194번\n",
      "다투: 194번\n",
      "다르: 193번\n",
      "욕: 193번\n",
      "노력: 193번\n",
      "빌리: 193번\n",
      "안되: 192번\n",
      "제거제: 191번\n",
      "뛰: 191번\n",
      "노인: 191번\n",
      "약: 190번\n",
      "다녀오: 190번\n",
      "가기: 189번\n",
      "구들: 189번\n",
      "비싸: 189번\n",
      "당혹: 189번\n",
      "모습: 188번\n",
      "끊어지: 188번\n",
      "몰르: 186번\n",
      "닿: 186번\n",
      "아무리: 185번\n",
      "차: 185번\n",
      "무기력: 185번\n",
      "경제적: 185번\n",
      "너무나: 184번\n",
      "계획: 184번\n",
      "도움: 184번\n",
      "기다리: 184번\n",
      "편안: 184번\n",
      "스: 183번\n",
      "제대로: 183번\n",
      "밥: 183번\n",
      "보험: 182번\n",
      "근데: 181번\n",
      "옆: 181번\n",
      "출산: 181번\n",
      "느낌: 180번\n",
      "부족: 180번\n",
      "발표: 180번\n",
      "맨날: 179번\n",
      "덜: 179번\n",
      "삶: 179번\n",
      "헤어지: 179번\n",
      "생일: 179번\n",
      "관리: 177번\n",
      "만족: 176번\n",
      "미안: 175번\n",
      "사라지: 175번\n",
      "지원: 175번\n",
      "사귀: 174번\n",
      "사업: 174번\n",
      "쉽: 172번\n",
      "줄: 172번\n",
      "다음: 172번\n",
      "모임: 171번\n",
      "서럽: 171번\n",
      "잔소리: 170번\n",
      "먼저: 170번\n",
      "제일: 170번\n",
      "자녀: 170번\n",
      "아무래도: 168번\n",
      "빠지: 168번\n",
      "용돈: 168번\n",
      "자랑: 168번\n",
      "당뇨: 168번\n",
      "투자: 167번\n",
      "키우: 167번\n",
      "돌보: 167번\n",
      "잃: 165번\n",
      "나랑: 165번\n",
      "새: 164번\n",
      "어디: 163번\n",
      "천둥: 162번\n",
      "구역질: 162번\n",
      "임신: 162번\n",
      "어쩌: 161번\n",
      "김: 161번\n",
      "새로: 161번\n",
      "관심: 161번\n",
      "하나: 160번\n",
      "매번: 160번\n",
      "뒤: 160번\n",
      "상태: 160번\n",
      "매우: 160번\n",
      "출근: 159번\n",
      "낮: 159번\n",
      "물어보: 158번\n",
      "같애: 157번\n",
      "눈: 157번\n",
      "넘: 157번\n",
      "느: 157번\n",
      "팔: 157번\n",
      "졸업: 157번\n",
      "입원: 157번\n",
      "옆집: 157번\n",
      "사랑: 156번\n",
      "집중: 156번\n",
      "갱신: 155번\n",
      "점수: 155번\n",
      "아버지: 155번\n",
      "머리: 153번\n",
      "거지: 152번\n",
      "열: 152번\n",
      "동네: 152번\n",
      "팀장: 152번\n",
      "마비: 152번\n",
      "인하: 151번\n",
      "눈치: 150번\n",
      "며칠: 149번\n",
      "손해: 149번\n",
      "풀: 148번\n",
      "멀: 148번\n",
      "적: 148번\n",
      "소식: 147번\n",
      "감정: 147번\n",
      "타: 147번\n",
      "직업: 147번\n",
      "평소: 146번\n",
      "어머니: 146번\n",
      "대학교: 146번\n",
      "당연히: 145번\n",
      "끝내: 145번\n",
      "영상: 144번\n",
      "만들: 144번\n",
      "때리: 144번\n",
      "정신: 144번\n",
      "긴장: 144번\n",
      "한번: 143번\n"
     ]
    }
   ],
   "source": [
    "# 상위 500개의 단어 선택\n",
    "top_words = [word for word, count in word_counts.most_common(500)]\n",
    "\n",
    "# 상위 500개의 단어에 대한 빈도수 출력\n",
    "for word in top_words:\n",
    "    print(f'{word}: {word_counts[word]}번')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.050501755665138 8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "lengths = np.array([len(x) for x in num_tokenized_data])\n",
    "print(np.mean(lengths), np.median(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(lengths)\n",
    "plt.xlabel(\"length\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kikikiju\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "(52117, 10)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "train_seq = pad_sequences(num_tokenized_data, maxlen = 10)\n",
    "\n",
    "print(train_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_input, test_input, train_target, test_target = train_test_split(train_seq, target_list, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, val_input, train_target, val_target = train_test_split(train_input, train_target, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33354, 10)\n",
      "(8339, 10)\n",
      "(10424, 10)\n",
      "(33354,)\n",
      "(8339,)\n",
      "(10424,)\n"
     ]
    }
   ],
   "source": [
    "train_target = np.array(train_target)\n",
    "val_target = np.array(val_target)\n",
    "test_target = np.array(test_target)\n",
    "\n",
    "print(train_input.shape)\n",
    "print(val_input.shape)\n",
    "print(test_input.shape)\n",
    "\n",
    "print(train_target.shape)\n",
    "print(val_target.shape)\n",
    "print(test_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. 모델\n",
    "\n",
    "## 2-1. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kikikiju\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:From C:\\Users\\kikikiju\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kikikiju\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "131/131 [==============================] - 12s 47ms/step - loss: 1.4421 - accuracy: 0.3658 - val_loss: 1.3963 - val_accuracy: 0.2654\n",
      "Epoch 2/300\n",
      "  3/131 [..............................] - ETA: 5s - loss: 1.2572 - accuracy: 0.4062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kikikiju\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/131 [==============================] - 7s 54ms/step - loss: 1.2372 - accuracy: 0.4384 - val_loss: 1.3678 - val_accuracy: 0.2944\n",
      "Epoch 3/300\n",
      "131/131 [==============================] - 8s 63ms/step - loss: 1.1724 - accuracy: 0.4760 - val_loss: 1.2802 - val_accuracy: 0.3847\n",
      "Epoch 4/300\n",
      "131/131 [==============================] - 7s 54ms/step - loss: 1.1335 - accuracy: 0.4983 - val_loss: 1.1393 - val_accuracy: 0.5292\n",
      "Epoch 5/300\n",
      "131/131 [==============================] - 6s 49ms/step - loss: 1.0975 - accuracy: 0.5213 - val_loss: 1.0423 - val_accuracy: 0.5663\n",
      "Epoch 6/300\n",
      "131/131 [==============================] - 7s 54ms/step - loss: 1.0728 - accuracy: 0.5355 - val_loss: 0.9803 - val_accuracy: 0.5953\n",
      "Epoch 7/300\n",
      "131/131 [==============================] - 8s 59ms/step - loss: 1.0561 - accuracy: 0.5464 - val_loss: 0.9656 - val_accuracy: 0.5912\n",
      "Epoch 8/300\n",
      "131/131 [==============================] - 7s 55ms/step - loss: 1.0427 - accuracy: 0.5542 - val_loss: 0.9418 - val_accuracy: 0.6058\n",
      "Epoch 9/300\n",
      "131/131 [==============================] - 8s 62ms/step - loss: 1.0245 - accuracy: 0.5672 - val_loss: 0.9401 - val_accuracy: 0.6082\n",
      "Epoch 10/300\n",
      "131/131 [==============================] - 8s 62ms/step - loss: 1.0106 - accuracy: 0.5746 - val_loss: 0.9324 - val_accuracy: 0.6083\n",
      "Epoch 11/300\n",
      "131/131 [==============================] - 7s 56ms/step - loss: 1.0003 - accuracy: 0.5781 - val_loss: 0.9142 - val_accuracy: 0.6172\n",
      "Epoch 12/300\n",
      "131/131 [==============================] - 8s 62ms/step - loss: 0.9902 - accuracy: 0.5865 - val_loss: 0.9010 - val_accuracy: 0.6229\n",
      "Epoch 13/300\n",
      "131/131 [==============================] - 7s 56ms/step - loss: 0.9803 - accuracy: 0.5898 - val_loss: 0.8947 - val_accuracy: 0.6259\n",
      "Epoch 14/300\n",
      "131/131 [==============================] - 7s 56ms/step - loss: 0.9696 - accuracy: 0.5955 - val_loss: 0.8869 - val_accuracy: 0.6280\n",
      "Epoch 15/300\n",
      "131/131 [==============================] - 7s 55ms/step - loss: 0.9648 - accuracy: 0.5991 - val_loss: 0.8811 - val_accuracy: 0.6317\n",
      "Epoch 16/300\n",
      "131/131 [==============================] - 7s 51ms/step - loss: 0.9519 - accuracy: 0.6018 - val_loss: 0.8718 - val_accuracy: 0.6329\n",
      "Epoch 17/300\n",
      "131/131 [==============================] - 7s 54ms/step - loss: 0.9432 - accuracy: 0.6055 - val_loss: 0.8704 - val_accuracy: 0.6376\n",
      "Epoch 18/300\n",
      "131/131 [==============================] - 7s 52ms/step - loss: 0.9395 - accuracy: 0.6136 - val_loss: 0.8643 - val_accuracy: 0.6438\n",
      "Epoch 19/300\n",
      "131/131 [==============================] - 7s 52ms/step - loss: 0.9388 - accuracy: 0.6145 - val_loss: 0.8608 - val_accuracy: 0.6395\n",
      "Epoch 20/300\n",
      "131/131 [==============================] - 7s 57ms/step - loss: 0.9319 - accuracy: 0.6150 - val_loss: 0.8542 - val_accuracy: 0.6452\n",
      "Epoch 21/300\n",
      "131/131 [==============================] - 7s 51ms/step - loss: 0.9284 - accuracy: 0.6169 - val_loss: 0.8484 - val_accuracy: 0.6436\n",
      "Epoch 22/300\n",
      "131/131 [==============================] - 7s 56ms/step - loss: 0.9193 - accuracy: 0.6210 - val_loss: 0.8459 - val_accuracy: 0.6509\n",
      "Epoch 23/300\n",
      "131/131 [==============================] - 7s 56ms/step - loss: 0.9166 - accuracy: 0.6200 - val_loss: 0.8435 - val_accuracy: 0.6456\n",
      "Epoch 24/300\n",
      "131/131 [==============================] - 8s 57ms/step - loss: 0.9110 - accuracy: 0.6245 - val_loss: 0.8400 - val_accuracy: 0.6491\n",
      "Epoch 25/300\n",
      "131/131 [==============================] - 8s 58ms/step - loss: 0.9056 - accuracy: 0.6312 - val_loss: 0.8382 - val_accuracy: 0.6483\n",
      "Epoch 26/300\n",
      "131/131 [==============================] - 7s 56ms/step - loss: 0.9049 - accuracy: 0.6306 - val_loss: 0.8362 - val_accuracy: 0.6531\n",
      "Epoch 27/300\n",
      "131/131 [==============================] - 7s 55ms/step - loss: 0.8990 - accuracy: 0.6315 - val_loss: 0.8352 - val_accuracy: 0.6480\n",
      "Epoch 28/300\n",
      "131/131 [==============================] - 7s 53ms/step - loss: 0.8997 - accuracy: 0.6330 - val_loss: 0.8310 - val_accuracy: 0.6524\n",
      "Epoch 29/300\n",
      "131/131 [==============================] - 7s 51ms/step - loss: 0.8925 - accuracy: 0.6346 - val_loss: 0.8649 - val_accuracy: 0.6380\n",
      "Epoch 30/300\n",
      "131/131 [==============================] - 7s 55ms/step - loss: 0.8909 - accuracy: 0.6367 - val_loss: 0.8356 - val_accuracy: 0.6502\n",
      "Epoch 31/300\n",
      "131/131 [==============================] - 7s 52ms/step - loss: 0.8843 - accuracy: 0.6368 - val_loss: 0.8234 - val_accuracy: 0.6585\n",
      "Epoch 32/300\n",
      "131/131 [==============================] - 7s 53ms/step - loss: 0.8864 - accuracy: 0.6385 - val_loss: 0.8176 - val_accuracy: 0.6597\n",
      "Epoch 33/300\n",
      "131/131 [==============================] - 7s 56ms/step - loss: 0.8822 - accuracy: 0.6396 - val_loss: 0.8202 - val_accuracy: 0.6586\n",
      "Epoch 34/300\n",
      "131/131 [==============================] - 7s 52ms/step - loss: 0.8798 - accuracy: 0.6430 - val_loss: 0.8184 - val_accuracy: 0.6598\n",
      "Epoch 35/300\n",
      "131/131 [==============================] - 8s 59ms/step - loss: 0.8773 - accuracy: 0.6435 - val_loss: 0.8162 - val_accuracy: 0.6593\n",
      "Epoch 36/300\n",
      "131/131 [==============================] - 7s 55ms/step - loss: 0.8749 - accuracy: 0.6459 - val_loss: 0.8142 - val_accuracy: 0.6612\n",
      "Epoch 37/300\n",
      "131/131 [==============================] - 7s 52ms/step - loss: 0.8707 - accuracy: 0.6484 - val_loss: 0.8310 - val_accuracy: 0.6503\n",
      "Epoch 38/300\n",
      "131/131 [==============================] - 7s 51ms/step - loss: 0.8728 - accuracy: 0.6486 - val_loss: 0.8230 - val_accuracy: 0.6568\n",
      "Epoch 39/300\n",
      "131/131 [==============================] - 7s 51ms/step - loss: 0.8719 - accuracy: 0.6483 - val_loss: 0.8095 - val_accuracy: 0.6635\n",
      "Epoch 40/300\n",
      "131/131 [==============================] - 7s 53ms/step - loss: 0.8684 - accuracy: 0.6516 - val_loss: 0.8117 - val_accuracy: 0.6646\n",
      "Epoch 41/300\n",
      "131/131 [==============================] - 7s 52ms/step - loss: 0.8647 - accuracy: 0.6485 - val_loss: 0.8054 - val_accuracy: 0.6665\n",
      "Epoch 42/300\n",
      "131/131 [==============================] - 7s 54ms/step - loss: 0.8625 - accuracy: 0.6516 - val_loss: 0.8035 - val_accuracy: 0.6677\n",
      "Epoch 43/300\n",
      "131/131 [==============================] - 7s 55ms/step - loss: 0.8634 - accuracy: 0.6522 - val_loss: 0.8214 - val_accuracy: 0.6492\n",
      "Epoch 44/300\n",
      "131/131 [==============================] - 7s 51ms/step - loss: 0.8575 - accuracy: 0.6537 - val_loss: 0.8056 - val_accuracy: 0.6655\n",
      "Epoch 45/300\n",
      "131/131 [==============================] - 7s 51ms/step - loss: 0.8582 - accuracy: 0.6539 - val_loss: 0.8168 - val_accuracy: 0.6562\n",
      "Epoch 46/300\n",
      "131/131 [==============================] - 7s 51ms/step - loss: 0.8572 - accuracy: 0.6556 - val_loss: 0.8079 - val_accuracy: 0.6609\n",
      "Epoch 47/300\n",
      "131/131 [==============================] - 7s 51ms/step - loss: 0.8614 - accuracy: 0.6558 - val_loss: 0.8134 - val_accuracy: 0.6588\n",
      "Epoch 48/300\n",
      "131/131 [==============================] - 7s 53ms/step - loss: 0.8553 - accuracy: 0.6544 - val_loss: 0.8050 - val_accuracy: 0.6681\n",
      "Epoch 49/300\n",
      "131/131 [==============================] - 8s 58ms/step - loss: 0.8530 - accuracy: 0.6614 - val_loss: 0.7963 - val_accuracy: 0.6689\n",
      "Epoch 50/300\n",
      "131/131 [==============================] - 8s 59ms/step - loss: 0.8516 - accuracy: 0.6576 - val_loss: 0.8375 - val_accuracy: 0.6462\n",
      "Epoch 51/300\n",
      "131/131 [==============================] - 7s 57ms/step - loss: 0.8477 - accuracy: 0.6612 - val_loss: 0.8301 - val_accuracy: 0.6444\n",
      "Epoch 52/300\n",
      "131/131 [==============================] - 8s 62ms/step - loss: 0.8487 - accuracy: 0.6575 - val_loss: 0.7935 - val_accuracy: 0.6691\n",
      "Epoch 53/300\n",
      "131/131 [==============================] - 8s 63ms/step - loss: 0.8501 - accuracy: 0.6608 - val_loss: 0.7987 - val_accuracy: 0.6671\n",
      "Epoch 54/300\n",
      "131/131 [==============================] - 9s 71ms/step - loss: 0.8473 - accuracy: 0.6599 - val_loss: 0.8216 - val_accuracy: 0.6531\n",
      "Epoch 55/300\n",
      "131/131 [==============================] - 9s 68ms/step - loss: 0.8451 - accuracy: 0.6608 - val_loss: 0.8237 - val_accuracy: 0.6501\n",
      "Epoch 56/300\n",
      "131/131 [==============================] - 9s 67ms/step - loss: 0.8403 - accuracy: 0.6623 - val_loss: 0.8107 - val_accuracy: 0.6609\n",
      "Epoch 57/300\n",
      "131/131 [==============================] - 8s 65ms/step - loss: 0.8473 - accuracy: 0.6595 - val_loss: 0.8013 - val_accuracy: 0.6640\n",
      "Epoch 58/300\n",
      "131/131 [==============================] - 9s 68ms/step - loss: 0.8434 - accuracy: 0.6639 - val_loss: 0.7943 - val_accuracy: 0.6688\n",
      "Epoch 59/300\n",
      "131/131 [==============================] - 8s 63ms/step - loss: 0.8405 - accuracy: 0.6641 - val_loss: 0.7933 - val_accuracy: 0.6682\n",
      "Epoch 60/300\n",
      "131/131 [==============================] - 7s 57ms/step - loss: 0.8423 - accuracy: 0.6644 - val_loss: 0.7900 - val_accuracy: 0.6701\n",
      "Epoch 61/300\n",
      "131/131 [==============================] - 7s 55ms/step - loss: 0.8392 - accuracy: 0.6627 - val_loss: 0.8056 - val_accuracy: 0.6612\n",
      "Epoch 62/300\n",
      "131/131 [==============================] - 7s 54ms/step - loss: 0.8367 - accuracy: 0.6651 - val_loss: 0.8020 - val_accuracy: 0.6655\n",
      "Epoch 63/300\n",
      "131/131 [==============================] - 7s 55ms/step - loss: 0.8395 - accuracy: 0.6650 - val_loss: 0.7879 - val_accuracy: 0.6702\n",
      "Epoch 64/300\n",
      "131/131 [==============================] - 7s 56ms/step - loss: 0.8404 - accuracy: 0.6644 - val_loss: 0.8158 - val_accuracy: 0.6584\n",
      "Epoch 65/300\n",
      "131/131 [==============================] - 8s 63ms/step - loss: 0.8394 - accuracy: 0.6669 - val_loss: 0.7902 - val_accuracy: 0.6715\n",
      "Epoch 66/300\n",
      "131/131 [==============================] - 8s 60ms/step - loss: 0.8389 - accuracy: 0.6660 - val_loss: 0.8005 - val_accuracy: 0.6664\n",
      "Epoch 67/300\n",
      "131/131 [==============================] - 7s 55ms/step - loss: 0.8338 - accuracy: 0.6659 - val_loss: 0.7929 - val_accuracy: 0.6676\n",
      "Epoch 68/300\n",
      "131/131 [==============================] - 7s 54ms/step - loss: 0.8353 - accuracy: 0.6656 - val_loss: 0.8068 - val_accuracy: 0.6598\n",
      "Epoch 69/300\n",
      "131/131 [==============================] - 7s 56ms/step - loss: 0.8343 - accuracy: 0.6657 - val_loss: 0.8135 - val_accuracy: 0.6610\n",
      "Epoch 70/300\n",
      "131/131 [==============================] - 7s 55ms/step - loss: 0.8326 - accuracy: 0.6675 - val_loss: 0.8077 - val_accuracy: 0.6584\n",
      "Epoch 71/300\n",
      "131/131 [==============================] - 8s 59ms/step - loss: 0.8281 - accuracy: 0.6702 - val_loss: 0.7881 - val_accuracy: 0.6723\n",
      "Epoch 72/300\n",
      "131/131 [==============================] - 6s 50ms/step - loss: 0.8308 - accuracy: 0.6678 - val_loss: 0.7973 - val_accuracy: 0.6670\n",
      "Epoch 73/300\n",
      "131/131 [==============================] - 6s 48ms/step - loss: 0.8330 - accuracy: 0.6674 - val_loss: 0.7901 - val_accuracy: 0.6711\n",
      "Epoch 74/300\n",
      "131/131 [==============================] - 6s 47ms/step - loss: 0.8296 - accuracy: 0.6679 - val_loss: 0.7884 - val_accuracy: 0.6725\n",
      "Epoch 75/300\n",
      "131/131 [==============================] - 6s 47ms/step - loss: 0.8287 - accuracy: 0.6700 - val_loss: 0.7993 - val_accuracy: 0.6658\n",
      "Epoch 76/300\n",
      "131/131 [==============================] - 6s 47ms/step - loss: 0.8305 - accuracy: 0.6696 - val_loss: 0.8015 - val_accuracy: 0.6636\n",
      "Epoch 77/300\n",
      "131/131 [==============================] - 7s 51ms/step - loss: 0.8306 - accuracy: 0.6695 - val_loss: 0.8206 - val_accuracy: 0.6561\n",
      "Epoch 78/300\n",
      "131/131 [==============================] - 6s 47ms/step - loss: 0.8248 - accuracy: 0.6719 - val_loss: 0.8276 - val_accuracy: 0.6554\n",
      "Epoch 79/300\n",
      "131/131 [==============================] - 6s 47ms/step - loss: 0.8266 - accuracy: 0.6681 - val_loss: 0.7893 - val_accuracy: 0.6708\n",
      "Epoch 80/300\n",
      "131/131 [==============================] - 6s 47ms/step - loss: 0.8248 - accuracy: 0.6715 - val_loss: 0.7956 - val_accuracy: 0.6658\n",
      "Epoch 81/300\n",
      "131/131 [==============================] - 6s 47ms/step - loss: 0.8246 - accuracy: 0.6680 - val_loss: 0.8111 - val_accuracy: 0.6599\n",
      "Epoch 82/300\n",
      "131/131 [==============================] - 6s 47ms/step - loss: 0.8279 - accuracy: 0.6710 - val_loss: 0.7823 - val_accuracy: 0.6774\n",
      "Epoch 83/300\n",
      "131/131 [==============================] - 7s 50ms/step - loss: 0.8235 - accuracy: 0.6719 - val_loss: 0.7966 - val_accuracy: 0.6663\n",
      "Epoch 84/300\n",
      "131/131 [==============================] - 6s 47ms/step - loss: 0.8217 - accuracy: 0.6739 - val_loss: 0.7848 - val_accuracy: 0.6755\n",
      "Epoch 85/300\n",
      "131/131 [==============================] - 6s 47ms/step - loss: 0.8240 - accuracy: 0.6725 - val_loss: 0.7901 - val_accuracy: 0.6705\n",
      "Epoch 86/300\n",
      "131/131 [==============================] - 7s 50ms/step - loss: 0.8256 - accuracy: 0.6728 - val_loss: 0.7794 - val_accuracy: 0.6738\n",
      "Epoch 87/300\n",
      "131/131 [==============================] - 6s 46ms/step - loss: 0.8243 - accuracy: 0.6717 - val_loss: 0.7827 - val_accuracy: 0.6733\n",
      "Epoch 88/300\n",
      "131/131 [==============================] - 6s 45ms/step - loss: 0.8194 - accuracy: 0.6734 - val_loss: 0.7949 - val_accuracy: 0.6661\n",
      "Epoch 89/300\n",
      "131/131 [==============================] - 6s 46ms/step - loss: 0.8164 - accuracy: 0.6751 - val_loss: 0.7866 - val_accuracy: 0.6720\n",
      "Epoch 90/300\n",
      "131/131 [==============================] - 6s 44ms/step - loss: 0.8203 - accuracy: 0.6722 - val_loss: 0.7818 - val_accuracy: 0.6733\n",
      "Epoch 91/300\n",
      "131/131 [==============================] - 6s 45ms/step - loss: 0.8167 - accuracy: 0.6753 - val_loss: 0.8095 - val_accuracy: 0.6579\n",
      "Epoch 92/300\n",
      "131/131 [==============================] - 6s 47ms/step - loss: 0.8219 - accuracy: 0.6728 - val_loss: 0.8017 - val_accuracy: 0.6634\n",
      "Epoch 93/300\n",
      "131/131 [==============================] - 6s 44ms/step - loss: 0.8156 - accuracy: 0.6736 - val_loss: 0.7891 - val_accuracy: 0.6720\n",
      "Epoch 94/300\n",
      "131/131 [==============================] - 6s 46ms/step - loss: 0.8209 - accuracy: 0.6734 - val_loss: 0.7910 - val_accuracy: 0.6645\n",
      "Epoch 95/300\n",
      "131/131 [==============================] - 6s 46ms/step - loss: 0.8178 - accuracy: 0.6744 - val_loss: 0.7828 - val_accuracy: 0.6745\n",
      "Epoch 96/300\n",
      "131/131 [==============================] - 6s 46ms/step - loss: 0.8171 - accuracy: 0.6779 - val_loss: 0.7810 - val_accuracy: 0.6751\n",
      "Epoch 97/300\n",
      "131/131 [==============================] - 6s 45ms/step - loss: 0.8179 - accuracy: 0.6736 - val_loss: 0.7976 - val_accuracy: 0.6646\n",
      "Epoch 98/300\n",
      "131/131 [==============================] - 6s 46ms/step - loss: 0.8151 - accuracy: 0.6749 - val_loss: 0.7832 - val_accuracy: 0.6742\n",
      "Epoch 99/300\n",
      "131/131 [==============================] - 6s 45ms/step - loss: 0.8137 - accuracy: 0.6756 - val_loss: 0.7880 - val_accuracy: 0.6703\n",
      "Epoch 100/300\n",
      "131/131 [==============================] - 6s 45ms/step - loss: 0.8159 - accuracy: 0.6739 - val_loss: 0.7853 - val_accuracy: 0.6736\n",
      "Epoch 101/300\n",
      "131/131 [==============================] - 6s 47ms/step - loss: 0.8154 - accuracy: 0.6740 - val_loss: 0.7844 - val_accuracy: 0.6735\n",
      "Epoch 102/300\n",
      "131/131 [==============================] - 6s 48ms/step - loss: 0.8127 - accuracy: 0.6763 - val_loss: 0.7818 - val_accuracy: 0.6771\n",
      "Epoch 103/300\n",
      "131/131 [==============================] - 6s 44ms/step - loss: 0.8136 - accuracy: 0.6756 - val_loss: 0.7854 - val_accuracy: 0.6769\n",
      "Epoch 104/300\n",
      "131/131 [==============================] - 6s 46ms/step - loss: 0.8136 - accuracy: 0.6775 - val_loss: 0.7792 - val_accuracy: 0.6760\n",
      "Epoch 105/300\n",
      "131/131 [==============================] - 6s 47ms/step - loss: 0.8147 - accuracy: 0.6776 - val_loss: 0.7790 - val_accuracy: 0.6756\n",
      "Epoch 106/300\n",
      "131/131 [==============================] - 6s 48ms/step - loss: 0.8140 - accuracy: 0.6765 - val_loss: 0.7763 - val_accuracy: 0.6768\n",
      "Epoch 107/300\n",
      "131/131 [==============================] - 6s 49ms/step - loss: 0.8119 - accuracy: 0.6769 - val_loss: 0.7767 - val_accuracy: 0.6775\n",
      "Epoch 108/300\n",
      "131/131 [==============================] - 6s 45ms/step - loss: 0.8083 - accuracy: 0.6777 - val_loss: 0.7924 - val_accuracy: 0.6660\n",
      "Epoch 109/300\n",
      "131/131 [==============================] - 6s 46ms/step - loss: 0.8134 - accuracy: 0.6748 - val_loss: 0.8022 - val_accuracy: 0.6649\n",
      "Epoch 110/300\n",
      "131/131 [==============================] - 6s 46ms/step - loss: 0.8109 - accuracy: 0.6778 - val_loss: 0.7768 - val_accuracy: 0.6772\n",
      "Epoch 111/300\n",
      "131/131 [==============================] - 6s 46ms/step - loss: 0.8078 - accuracy: 0.6779 - val_loss: 0.7907 - val_accuracy: 0.6679\n",
      "Epoch 112/300\n",
      "131/131 [==============================] - 7s 53ms/step - loss: 0.8098 - accuracy: 0.6781 - val_loss: 0.8054 - val_accuracy: 0.6627\n",
      "Epoch 113/300\n",
      "131/131 [==============================] - 6s 47ms/step - loss: 0.8091 - accuracy: 0.6775 - val_loss: 0.8000 - val_accuracy: 0.6636\n",
      "Epoch 114/300\n",
      "131/131 [==============================] - 6s 46ms/step - loss: 0.8111 - accuracy: 0.6774 - val_loss: 0.7946 - val_accuracy: 0.6690\n",
      "Epoch 115/300\n",
      "131/131 [==============================] - 6s 45ms/step - loss: 0.8073 - accuracy: 0.6782 - val_loss: 0.7766 - val_accuracy: 0.6751\n",
      "Epoch 116/300\n",
      "131/131 [==============================] - 6s 45ms/step - loss: 0.8071 - accuracy: 0.6780 - val_loss: 0.8071 - val_accuracy: 0.6625\n",
      "Epoch 117/300\n",
      "131/131 [==============================] - 6s 47ms/step - loss: 0.8097 - accuracy: 0.6781 - val_loss: 0.7974 - val_accuracy: 0.6639\n",
      "Epoch 118/300\n",
      "131/131 [==============================] - 6s 45ms/step - loss: 0.8071 - accuracy: 0.6802 - val_loss: 0.7904 - val_accuracy: 0.6694\n",
      "Epoch 119/300\n",
      "131/131 [==============================] - 7s 53ms/step - loss: 0.8060 - accuracy: 0.6806 - val_loss: 0.7825 - val_accuracy: 0.6736\n",
      "Epoch 120/300\n",
      "131/131 [==============================] - 7s 51ms/step - loss: 0.8047 - accuracy: 0.6778 - val_loss: 0.7951 - val_accuracy: 0.6660\n",
      "Epoch 121/300\n",
      "131/131 [==============================] - 8s 59ms/step - loss: 0.8051 - accuracy: 0.6791 - val_loss: 0.7848 - val_accuracy: 0.6742\n",
      "Epoch 122/300\n",
      "131/131 [==============================] - 8s 61ms/step - loss: 0.8061 - accuracy: 0.6790 - val_loss: 0.7922 - val_accuracy: 0.6682\n",
      "Epoch 123/300\n",
      "131/131 [==============================] - 7s 57ms/step - loss: 0.8042 - accuracy: 0.6793 - val_loss: 0.7903 - val_accuracy: 0.6681\n",
      "Epoch 124/300\n",
      "131/131 [==============================] - 7s 54ms/step - loss: 0.8066 - accuracy: 0.6791 - val_loss: 0.7889 - val_accuracy: 0.6687\n",
      "Epoch 125/300\n",
      "131/131 [==============================] - 7s 54ms/step - loss: 0.8038 - accuracy: 0.6787 - val_loss: 0.7970 - val_accuracy: 0.6575\n",
      "Epoch 126/300\n",
      "131/131 [==============================] - 7s 55ms/step - loss: 0.8048 - accuracy: 0.6789 - val_loss: 0.7838 - val_accuracy: 0.6717\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "# 모델 정의\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=500, output_dim=128, input_length=10))\n",
    "model.add(GRU(128, dropout=0.3, recurrent_dropout=0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(4, activation='softmax')) # 클래스의 개수에 맞게 설정\n",
    "\n",
    "\n",
    "initial_learning_rate = 0.01\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps=10000, decay_rate=0.9)\n",
    "\n",
    "optimizer = SGD(learning_rate=lr_schedule)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 콜백 정의\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-embedding-model.h5', save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(train_input, train_target, epochs=300, batch_size=256,\n",
    "                    validation_data=(val_input, val_target),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326/326 [==============================] - 1s 3ms/step - loss: 0.7855 - accuracy: 0.6709\n",
      "Test Loss: 0.785527229309082\n",
      "Test Accuracy: 0.670855700969696\n"
     ]
    }
   ],
   "source": [
    "# 패딩이 완료된 테스트 데이터\n",
    "test_input = pad_sequences(test_input, maxlen=8)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_input, test_target)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3. 문장에서 감정분석 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "입력 문장: 엄마한테 혼났어\n",
      "예측된 감정: 슬픔\n",
      "각 클래스의 확률: [0.27343827 0.3478831  0.25848356 0.12019516]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "입력 문장: 엄마한테 용돈받았어\n",
      "예측된 감정: 행복\n",
      "각 클래스의 확률: [0.15158467 0.29469588 0.09977803 0.45394143]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "입력 문장: 시험 잘 봤을까\n",
      "예측된 감정: 분노\n",
      "각 클래스의 확률: [0.27869982 0.25073212 0.21561885 0.2549492 ]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "입력 문장: 시험 잘 봤어\n",
      "예측된 감정: 분노\n",
      "각 클래스의 확률: [0.27869982 0.25073212 0.21561885 0.2549492 ]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "# Assuming 'model' and 'word_to_index' are defined before this code\n",
    "\n",
    "# Load the Kkma tokenizer\n",
    "kkma = Kkma()\n",
    "\n",
    "def tokenize_and_pad(sentence, tokenizer, word_to_index, max_length=10):\n",
    "    input_tokens = [word for word, pos in tokenizer.pos(sentence) if (pos == 'NNG') or (pos == 'VV') or (pos == 'NP') or (pos == 'VA') or (pos == 'MAG')]\n",
    "    input_indices = [word_to_index.get(word, 0) for word in input_tokens]\n",
    "    padded_input = pad_sequences([input_indices], maxlen=max_length)\n",
    "    return padded_input\n",
    "\n",
    "# 사용 예시\n",
    "while True:\n",
    "    input_sentence = input()\n",
    "    if input_sentence == '0':\n",
    "        break\n",
    "\n",
    "    padded_input = tokenize_and_pad(input_sentence, kkma, word_to_index)\n",
    "\n",
    "    predicted_probabilities = model.predict(padded_input)\n",
    "    predicted_class = np.argmax(predicted_probabilities)\n",
    "\n",
    "    emotions = ['분노', '슬픔', '불안', '행복']\n",
    "    predicted_emotion = emotions[predicted_class]\n",
    "\n",
    "    print(f\"입력 문장: {input_sentence}\")\n",
    "    print(f\"예측된 감정: {predicted_emotion}\")\n",
    "    print(f\"각 클래스의 확률: {predicted_probabilities[0]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
